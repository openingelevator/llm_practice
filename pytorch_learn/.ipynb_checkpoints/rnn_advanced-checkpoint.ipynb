{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe598eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gzip\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5407e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, is_train_set = True):\n",
    "        # 划分数据集\n",
    "        filename = 'names_train.csv.gz' if is_train_set else 'names_test.csv.gz'\n",
    "        with gzip.open(filename, 'rt') as f:\n",
    "            reader = csv.reader(f)\n",
    "            rows = list(reader)\n",
    "        # 姓名序列\n",
    "        self.names = [row[0] for row in rows]\n",
    "        # 姓名序列的长度，即总的样本数\n",
    "        self.len = len(self.names)\n",
    "        # 国家序列\n",
    "        self.countries = [row[1] for row in rows]\n",
    "        # 去除国家序列中的重复值，并进行排序，以便后续将国家的字符串映射到对应的数值\n",
    "        self.country_list = list(sorted(set(self.countries)))\n",
    "        # 国家字符串对应字典，以数字作为value值\n",
    "        self.country_dict = self.getCountryDict()\n",
    "        # 总的国家数，即分类的类别数量\n",
    "        self.country_num = len(self.country_list)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # dataloader每次所取的一条记录，返回（姓名，对应国家的id值）\n",
    "        return self.names[index], self.country_dict[self.countries[index]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def getCountryDict(self):\n",
    "        # 创建空字典\n",
    "        country_dict = dict()\n",
    "        # 遍历国家列表\n",
    "        for idx, country_name in enumerate(self.country_list, 0):\n",
    "            # 将国家字符串作为key值，idx作为value值\n",
    "            country_dict[country_name] = idx\n",
    "        return country_dict\n",
    "    \n",
    "    def idx2country(self, index):\n",
    "        # 返回索引对应的国家字符串\n",
    "        return self.country_list[index]\n",
    "    \n",
    "    def getCountriesNum(self):\n",
    "        return self.country_num\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "596162aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 256\n",
    "N_LAYER = 2\n",
    "N_EPOCHS = 100\n",
    "N_CHARS = 128\n",
    "USE_GPU = True\n",
    "\n",
    "trainset = Dataset(is_train_set=True)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testset = Dataset(is_train_set=False)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "N_COUNTRY = trainset.getCountriesNum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efd84f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor(tensor):\n",
    "    # 将张量计算迁移到GPU上\n",
    "    if USE_GPU:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        tensor = tensor.to(device)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca13f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers = 1, bidirectional = True):\n",
    "        '''\n",
    "        input_size: 输入维度，输入为字符对应的128维独热向量\n",
    "        hidden_size: 隐藏层的维度，\n",
    "        output_size: 全连接层的输出维度，表示输出的类别\n",
    "        num_layers: gru层叠加的层数，决定了hidden向量的输出维度（num_layers×num_direction，batch_size，hidden_size）\n",
    "        bidirectional: 是否使用双向循环，如果使用，hidden向量的最终输出是由[h_N^f, h_N^b]组成的\n",
    "        \n",
    "        '''\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = num_layers\n",
    "        self.n_direction = 2 if bidirectional else 1\n",
    "        # 嵌入层，尺寸为（input_size，hidden_size）的矩阵\n",
    "        self.emb = torch.nn.Embedding(input_size, hidden_size)\n",
    "        # gru单元\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, num_layers, bidirectional = bidirectional)\n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_size*self.n_direction, output_size)\n",
    "        \n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers*self.n_direction, batch_size, self.hidden_size)\n",
    "        return create_tensor(hidden)\n",
    "        \n",
    "    def forward(self, inputs, seq_lengths):\n",
    "         '''\n",
    "        input_size: 输入向量\n",
    "        seq_lengths: 表示一个批次数据中，每个词向量对应的序列长度（单词长度），为数组格式\n",
    "        \n",
    "        '''\n",
    "        # （batch_size，seq_len）-> （seq_len，batch_size）\n",
    "        inputs = inputs.t()\n",
    "        batch_size = inputs.size(1)\n",
    "        \n",
    "        # 初始隐藏层向量，输出为（num_layers×num_direction，batch_size，hidden_size）\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        # 嵌入层向量，输出为（seq_len，batch_size, hidden_size）\n",
    "        embedding = self.emb(inputs)\n",
    "        # 由于输入的词向量长短不一，在输入时需要以最长的词向量长度为标准进行0填充\n",
    "        # 为了减少冗余的零计算，gru单元的输入可以调整为一种特殊的堆叠方式\n",
    "        # 具体操作为，首先将一个批次的数据进行转置，得到（seq_len，batch_size）格式的数据\n",
    "        # 之后对数据在dim=0维度上进行降序排序，就可以得到词向量从长到段的一块数据\n",
    "        # 在经过embedding层之后，可以得到（seq_len，batch_size, hidden_size）的数据\n",
    "        # 我们在dim=0维度上依次取出一块数据\n",
    "        # 假设第一行，也就是seq_len=0取出的数据中非零元素为10（小于等于batch_size）\n",
    "        # 依次类推，每一行取出的序列长度为[10,10,8,6,...,1]，这个数组大小为seq_len\n",
    "        # 然后将这些数据进行垂直方向堆叠，得到（sum([10,10,8,6,...,1]), 1, hidden_size）格式的数据\n",
    "        gru_input = torch.nn.utils.rnn.pack_padded_sequence(embedding, seq_lengths.to('cpu'))\n",
    "        output, hidden = self.gru(gru_input, hidden)\n",
    "        \n",
    "        if self.n_direction == 2:\n",
    "            hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim = 1)\n",
    "        else:\n",
    "            hidden_cat = hidden[-1]\n",
    "            \n",
    "        fc_output = self.fc(hidden_cat)\n",
    "        \n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e46b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name2list(name):\n",
    "    arr = [ord(c) for c in name]\n",
    "    return arr, len(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54a716b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensors(names, countries):\n",
    "    # 对读入的数据进行格式的转换处理\n",
    "    # 获取每个姓名对应词向量的长度以及ascill码值\n",
    "    sequences_and_length = [name2list(name) for name in names]\n",
    "    # 姓名序列（batch_size，不等长）\n",
    "    name_sequences = [sl[0] for sl in sequences_and_length]\n",
    "    # 序列长度数组（batch_size，1）\n",
    "    sequence_lengths = torch.LongTensor([sl[1] for sl in sequences_and_length])\n",
    "    countries = countries.long()\n",
    "    \n",
    "    # 生成（batch_size，最长序列长度）维度的零向量，用于统一单词向量的长度，进行0填充\n",
    "    seq_tensor = torch.zeros(len(name_sequences), sequence_lengths.max()).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(name_sequences, sequence_lengths), 0):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "    \n",
    "    # 对序列长度在seq_len维度上进行排序，方便进行pack_padded_sequence操作\n",
    "    sequence_lengths, perm_idx = sequence_lengths.sort(dim = 0, descending = True)\n",
    "    # 通过排序得到的位置索引张量，同时对词向量以及标签进行排序\n",
    "    seq_tensor = seq_tensor[perm_idx]\n",
    "    countries = countries[perm_idx]\n",
    "    \n",
    "    return create_tensor(seq_tensor), \\\n",
    "            create_tensor(sequence_lengths), \\\n",
    "            create_tensor(countries)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a5448f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq = torch.LongTensor([[1], [7], [2]])\n",
    "# seq_tensor = torch.LongTensor([[1,4,3,0],[2,4,5,0],[3,3,0,0]])\n",
    "# seq , perm_idx = seq.sort(dim = 0, descending = True)\n",
    "# print(seq)\n",
    "# print(perm_idx)\n",
    "# print(seq_tensor[perm_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99bdd7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel():\n",
    "    total_loss = 0\n",
    "    for i, (names, countries) in enumerate(trainloader, 1):\n",
    "        inputs, seq_lengths, target = make_tensors(names, countries)\n",
    "        output = classifier(inputs, seq_lengths)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(f'[{time_since(start)}] Epoch {epoch} ', end='')\n",
    "            print(f'[{i * len(inputs)}/{len(trainset)}] ', end='')\n",
    "            print(f'loss={total_loss / (i * len(inputs))}')\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c24e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel():\n",
    "    correct = 0\n",
    "    total = len(testset)\n",
    "    print(\"evaluating trained model ...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (names, countries) in enumerate(testloader, 1):\n",
    "            inputs, seq_lengths, target = make_tensors(names, countries)\n",
    "            output = classifier(inputs, seq_lengths)\n",
    "            pred = output.max(dim=1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "        percent = '%.2f' % (100 * correct / total)\n",
    "        print(f'Test set: Accuracy {correct}/{total} {percent}%')\n",
    "        \n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52e94c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "384945f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 epochs...\n",
      "[0m 0s] Epoch 1 [2560/13374] loss=0.008906270982697606\n",
      "[0m 0s] Epoch 1 [5120/13374] loss=0.007517404621466994\n",
      "[0m 0s] Epoch 1 [7680/13374] loss=0.006871616886928678\n",
      "[0m 1s] Epoch 1 [10240/13374] loss=0.0063803127035498616\n",
      "[0m 1s] Epoch 1 [12800/13374] loss=0.00602269172668457\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 4483/6700 66.91%\n",
      "[0m 1s] Epoch 2 [2560/13374] loss=0.004149026772938669\n",
      "[0m 2s] Epoch 2 [5120/13374] loss=0.004082870902493596\n",
      "[0m 2s] Epoch 2 [7680/13374] loss=0.00397601769460986\n",
      "[0m 2s] Epoch 2 [10240/13374] loss=0.003910147823626175\n",
      "[0m 2s] Epoch 2 [12800/13374] loss=0.0038455851888284085\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 4951/6700 73.90%\n",
      "[0m 3s] Epoch 3 [2560/13374] loss=0.0032419850816950203\n",
      "[0m 3s] Epoch 3 [5120/13374] loss=0.003175458393525332\n",
      "[0m 3s] Epoch 3 [7680/13374] loss=0.003116993559524417\n",
      "[0m 4s] Epoch 3 [10240/13374] loss=0.0030934904993046076\n",
      "[0m 4s] Epoch 3 [12800/13374] loss=0.0030478351702913644\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5161/6700 77.03%\n",
      "[0m 4s] Epoch 4 [2560/13374] loss=0.0027175817172974347\n",
      "[0m 4s] Epoch 4 [5120/13374] loss=0.0027108551585115492\n",
      "[0m 5s] Epoch 4 [7680/13374] loss=0.0026531903150801856\n",
      "[0m 5s] Epoch 4 [10240/13374] loss=0.0026319197088014333\n",
      "[0m 5s] Epoch 4 [12800/13374] loss=0.0025960667012259364\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5338/6700 79.67%\n",
      "[0m 6s] Epoch 5 [2560/13374] loss=0.002244630944915116\n",
      "[0m 6s] Epoch 5 [5120/13374] loss=0.002228261053096503\n",
      "[0m 6s] Epoch 5 [7680/13374] loss=0.0022550479198495546\n",
      "[0m 6s] Epoch 5 [10240/13374] loss=0.002252234925981611\n",
      "[0m 7s] Epoch 5 [12800/13374] loss=0.0022519103274680676\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5412/6700 80.78%\n",
      "[0m 7s] Epoch 6 [2560/13374] loss=0.00212379836011678\n",
      "[0m 7s] Epoch 6 [5120/13374] loss=0.0019976627256255595\n",
      "[0m 8s] Epoch 6 [7680/13374] loss=0.0020195232626671594\n",
      "[0m 8s] Epoch 6 [10240/13374] loss=0.0019912846793886272\n",
      "[0m 8s] Epoch 6 [12800/13374] loss=0.0019914048467762767\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5492/6700 81.97%\n",
      "[0m 9s] Epoch 7 [2560/13374] loss=0.0018192728166468441\n",
      "[0m 9s] Epoch 7 [5120/13374] loss=0.0018172503856476395\n",
      "[0m 9s] Epoch 7 [7680/13374] loss=0.001803131898244222\n",
      "[0m 9s] Epoch 7 [10240/13374] loss=0.0018082683527609333\n",
      "[0m 9s] Epoch 7 [12800/13374] loss=0.001795540468301624\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5526/6700 82.48%\n",
      "[0m 10s] Epoch 8 [2560/13374] loss=0.001677962753456086\n",
      "[0m 10s] Epoch 8 [5120/13374] loss=0.0016637447581160814\n",
      "[0m 10s] Epoch 8 [7680/13374] loss=0.001647604303434491\n",
      "[0m 11s] Epoch 8 [10240/13374] loss=0.0016515829687705264\n",
      "[0m 11s] Epoch 8 [12800/13374] loss=0.0016351247089914978\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5603/6700 83.63%\n",
      "[0m 11s] Epoch 9 [2560/13374] loss=0.0014300044276751578\n",
      "[0m 11s] Epoch 9 [5120/13374] loss=0.0014539789000991732\n",
      "[0m 12s] Epoch 9 [7680/13374] loss=0.0014835507880585888\n",
      "[0m 12s] Epoch 9 [10240/13374] loss=0.0014819224568782374\n",
      "[0m 12s] Epoch 9 [12800/13374] loss=0.0014833258278667927\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5604/6700 83.64%\n",
      "[0m 13s] Epoch 10 [2560/13374] loss=0.0014657516730949283\n",
      "[0m 13s] Epoch 10 [5120/13374] loss=0.0014414463308639824\n",
      "[0m 13s] Epoch 10 [7680/13374] loss=0.0013953318120911717\n",
      "[0m 13s] Epoch 10 [10240/13374] loss=0.0013771735815680587\n",
      "[0m 13s] Epoch 10 [12800/13374] loss=0.0013717595872003586\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5628/6700 84.00%\n",
      "[0m 14s] Epoch 11 [2560/13374] loss=0.001068172074155882\n",
      "[0m 14s] Epoch 11 [5120/13374] loss=0.0010988027090206743\n",
      "[0m 14s] Epoch 11 [7680/13374] loss=0.0011433444257515173\n",
      "[0m 15s] Epoch 11 [10240/13374] loss=0.0011769137432565912\n",
      "[0m 15s] Epoch 11 [12800/13374] loss=0.001204171716235578\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5634/6700 84.09%\n",
      "[0m 15s] Epoch 12 [2560/13374] loss=0.001050065824529156\n",
      "[0m 15s] Epoch 12 [5120/13374] loss=0.001054251255118288\n",
      "[0m 16s] Epoch 12 [7680/13374] loss=0.001056397892534733\n",
      "[0m 16s] Epoch 12 [10240/13374] loss=0.001069000929419417\n",
      "[0m 16s] Epoch 12 [12800/13374] loss=0.0010985658585559576\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5621/6700 83.90%\n",
      "[0m 16s] Epoch 13 [2560/13374] loss=0.0009521954110823571\n",
      "[0m 17s] Epoch 13 [5120/13374] loss=0.000923111560405232\n",
      "[0m 17s] Epoch 13 [7680/13374] loss=0.0009455887076910586\n",
      "[0m 17s] Epoch 13 [10240/13374] loss=0.0009497413542703725\n",
      "[0m 17s] Epoch 13 [12800/13374] loss=0.0009737089416012168\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5656/6700 84.42%\n",
      "[0m 18s] Epoch 14 [2560/13374] loss=0.000789212214294821\n",
      "[0m 18s] Epoch 14 [5120/13374] loss=0.0008618355554062873\n",
      "[0m 18s] Epoch 14 [7680/13374] loss=0.0008662536410459627\n",
      "[0m 18s] Epoch 14 [10240/13374] loss=0.0008849813093547709\n",
      "[0m 18s] Epoch 14 [12800/13374] loss=0.0008746324945241213\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5665/6700 84.55%\n",
      "[0m 19s] Epoch 15 [2560/13374] loss=0.0006635731406277046\n",
      "[0m 19s] Epoch 15 [5120/13374] loss=0.0007073297645547427\n",
      "[0m 19s] Epoch 15 [7680/13374] loss=0.000728674346464686\n",
      "[0m 19s] Epoch 15 [10240/13374] loss=0.0007503940280003008\n",
      "[0m 20s] Epoch 15 [12800/13374] loss=0.0007579650386469439\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5650/6700 84.33%\n",
      "[0m 20s] Epoch 16 [2560/13374] loss=0.000651216006372124\n",
      "[0m 21s] Epoch 16 [5120/13374] loss=0.0006501619704067707\n",
      "[0m 21s] Epoch 16 [7680/13374] loss=0.0006630952537913496\n",
      "[0m 21s] Epoch 16 [10240/13374] loss=0.0006632925149460788\n",
      "[0m 21s] Epoch 16 [12800/13374] loss=0.0006681529321940616\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5638/6700 84.15%\n",
      "[0m 22s] Epoch 17 [2560/13374] loss=0.0005650855193380267\n",
      "[0m 22s] Epoch 17 [5120/13374] loss=0.0005899067517020739\n",
      "[0m 22s] Epoch 17 [7680/13374] loss=0.0006003652786603197\n",
      "[0m 23s] Epoch 17 [10240/13374] loss=0.0005991596714011394\n",
      "[0m 23s] Epoch 17 [12800/13374] loss=0.0006038684176746756\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5627/6700 83.99%\n",
      "[0m 24s] Epoch 18 [2560/13374] loss=0.00048214658454526216\n",
      "[0m 24s] Epoch 18 [5120/13374] loss=0.0005146237599547021\n",
      "[0m 24s] Epoch 18 [7680/13374] loss=0.0005363689871349682\n",
      "[0m 24s] Epoch 18 [10240/13374] loss=0.0005442689580377191\n",
      "[0m 25s] Epoch 18 [12800/13374] loss=0.0005436932155862451\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5605/6700 83.66%\n",
      "[0m 25s] Epoch 19 [2560/13374] loss=0.0004960273334290832\n",
      "[0m 26s] Epoch 19 [5120/13374] loss=0.00046903313195798544\n",
      "[0m 26s] Epoch 19 [7680/13374] loss=0.00046453141355110955\n",
      "[0m 26s] Epoch 19 [10240/13374] loss=0.00046933241974329577\n",
      "[0m 26s] Epoch 19 [12800/13374] loss=0.00048341318150050937\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5637/6700 84.13%\n",
      "[0m 27s] Epoch 20 [2560/13374] loss=0.0003630865758168511\n",
      "[0m 27s] Epoch 20 [5120/13374] loss=0.00038979048913461155\n",
      "[0m 27s] Epoch 20 [7680/13374] loss=0.0004051425984168115\n",
      "[0m 28s] Epoch 20 [10240/13374] loss=0.0004139468237553956\n",
      "[0m 28s] Epoch 20 [12800/13374] loss=0.0004194230303983204\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5633/6700 84.07%\n",
      "[0m 28s] Epoch 21 [2560/13374] loss=0.00034674604394240304\n",
      "[0m 29s] Epoch 21 [5120/13374] loss=0.00036845614304183984\n",
      "[0m 29s] Epoch 21 [7680/13374] loss=0.00036932238823889443\n",
      "[0m 29s] Epoch 21 [10240/13374] loss=0.00038429477863246577\n",
      "[0m 29s] Epoch 21 [12800/13374] loss=0.0003936406207503751\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5646/6700 84.27%\n",
      "[0m 30s] Epoch 22 [2560/13374] loss=0.00031947820680215954\n",
      "[0m 30s] Epoch 22 [5120/13374] loss=0.00032750284735811875\n",
      "[0m 30s] Epoch 22 [7680/13374] loss=0.0003450245218118653\n",
      "[0m 30s] Epoch 22 [10240/13374] loss=0.0003494657466944773\n",
      "[0m 31s] Epoch 22 [12800/13374] loss=0.00035689957032445816\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5641/6700 84.19%\n",
      "[0m 31s] Epoch 23 [2560/13374] loss=0.00029153404757380484\n",
      "[0m 31s] Epoch 23 [5120/13374] loss=0.0003129604236164596\n",
      "[0m 32s] Epoch 23 [7680/13374] loss=0.0003128448132580767\n",
      "[0m 32s] Epoch 23 [10240/13374] loss=0.00032108011655509474\n",
      "[0m 32s] Epoch 23 [12800/13374] loss=0.00033463072060840204\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5606/6700 83.67%\n",
      "[0m 33s] Epoch 24 [2560/13374] loss=0.000292587217700202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 33s] Epoch 24 [5120/13374] loss=0.0003190287920006085\n",
      "[0m 33s] Epoch 24 [7680/13374] loss=0.0003145210677757859\n",
      "[0m 33s] Epoch 24 [10240/13374] loss=0.00031885147909633816\n",
      "[0m 33s] Epoch 24 [12800/13374] loss=0.00032574844168266283\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5614/6700 83.79%\n",
      "[0m 34s] Epoch 25 [2560/13374] loss=0.000254477774433326\n",
      "[0m 34s] Epoch 25 [5120/13374] loss=0.00026628245032043194\n",
      "[0m 35s] Epoch 25 [7680/13374] loss=0.00027011349787547565\n",
      "[0m 35s] Epoch 25 [10240/13374] loss=0.000279885529744206\n",
      "[0m 35s] Epoch 25 [12800/13374] loss=0.00029479560267645864\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5617/6700 83.84%\n",
      "[0m 36s] Epoch 26 [2560/13374] loss=0.00023598355473950506\n",
      "[0m 36s] Epoch 26 [5120/13374] loss=0.0002482099946064409\n",
      "[0m 36s] Epoch 26 [7680/13374] loss=0.00026897233619820325\n",
      "[0m 36s] Epoch 26 [10240/13374] loss=0.00028273625466681553\n",
      "[0m 37s] Epoch 26 [12800/13374] loss=0.0002823653825907968\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5635/6700 84.10%\n",
      "[0m 37s] Epoch 27 [2560/13374] loss=0.00023710083187324926\n",
      "[0m 37s] Epoch 27 [5120/13374] loss=0.0002494014486728702\n",
      "[0m 38s] Epoch 27 [7680/13374] loss=0.00025309230986749756\n",
      "[0m 38s] Epoch 27 [10240/13374] loss=0.00026314620045013725\n",
      "[0m 38s] Epoch 27 [12800/13374] loss=0.0002766995449201204\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5603/6700 83.63%\n",
      "[0m 39s] Epoch 28 [2560/13374] loss=0.00023355221492238343\n",
      "[0m 39s] Epoch 28 [5120/13374] loss=0.00023042853135848417\n",
      "[0m 39s] Epoch 28 [7680/13374] loss=0.00022894896101206542\n",
      "[0m 39s] Epoch 28 [10240/13374] loss=0.00024097739551507403\n",
      "[0m 40s] Epoch 28 [12800/13374] loss=0.0002527920197462663\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5623/6700 83.93%\n",
      "[0m 40s] Epoch 29 [2560/13374] loss=0.00022317689799820073\n",
      "[0m 40s] Epoch 29 [5120/13374] loss=0.00021464715864567553\n",
      "[0m 41s] Epoch 29 [7680/13374] loss=0.0002276451217767317\n",
      "[0m 41s] Epoch 29 [10240/13374] loss=0.00024397577381023438\n",
      "[0m 41s] Epoch 29 [12800/13374] loss=0.00025640296735218726\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5603/6700 83.63%\n",
      "[0m 42s] Epoch 30 [2560/13374] loss=0.00024332634202437476\n",
      "[0m 42s] Epoch 30 [5120/13374] loss=0.0002346662586205639\n",
      "[0m 42s] Epoch 30 [7680/13374] loss=0.00023460174998035654\n",
      "[0m 43s] Epoch 30 [10240/13374] loss=0.00024074761677184142\n",
      "[0m 43s] Epoch 30 [12800/13374] loss=0.00024133179686032237\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5620/6700 83.88%\n",
      "[0m 43s] Epoch 31 [2560/13374] loss=0.00016941263602348043\n",
      "[0m 44s] Epoch 31 [5120/13374] loss=0.00020585311722243205\n",
      "[0m 44s] Epoch 31 [7680/13374] loss=0.00021688069246010854\n",
      "[0m 44s] Epoch 31 [10240/13374] loss=0.00021844969051016961\n",
      "[0m 44s] Epoch 31 [12800/13374] loss=0.00023250365629792213\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5611/6700 83.75%\n",
      "[0m 45s] Epoch 32 [2560/13374] loss=0.00019107503612758592\n",
      "[0m 45s] Epoch 32 [5120/13374] loss=0.00020080475296708756\n",
      "[0m 45s] Epoch 32 [7680/13374] loss=0.00021314452557514112\n",
      "[0m 46s] Epoch 32 [10240/13374] loss=0.00022024486133886967\n",
      "[0m 46s] Epoch 32 [12800/13374] loss=0.00022836657939478755\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5620/6700 83.88%\n",
      "[0m 46s] Epoch 33 [2560/13374] loss=0.00019818363289232367\n",
      "[0m 47s] Epoch 33 [5120/13374] loss=0.00019206449469493235\n",
      "[0m 47s] Epoch 33 [7680/13374] loss=0.00020063896251182694\n",
      "[0m 47s] Epoch 33 [10240/13374] loss=0.00021416497529571643\n",
      "[0m 47s] Epoch 33 [12800/13374] loss=0.000223894235532498\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5591/6700 83.45%\n",
      "[0m 48s] Epoch 34 [2560/13374] loss=0.00016046713426476344\n",
      "[0m 48s] Epoch 34 [5120/13374] loss=0.00016342144845111762\n",
      "[0m 48s] Epoch 34 [7680/13374] loss=0.0002106595020450186\n",
      "[0m 48s] Epoch 34 [10240/13374] loss=0.00021241214071778813\n",
      "[0m 49s] Epoch 34 [12800/13374] loss=0.00022924373493879102\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5623/6700 83.93%\n",
      "[0m 49s] Epoch 35 [2560/13374] loss=0.00024146272626239805\n",
      "[0m 50s] Epoch 35 [5120/13374] loss=0.00022514842385135125\n",
      "[0m 50s] Epoch 35 [7680/13374] loss=0.00022376020957987446\n",
      "[0m 50s] Epoch 35 [10240/13374] loss=0.00022260937566898065\n",
      "[0m 50s] Epoch 35 [12800/13374] loss=0.0002274242824933026\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5611/6700 83.75%\n",
      "[0m 51s] Epoch 36 [2560/13374] loss=0.00017600102073629388\n",
      "[0m 51s] Epoch 36 [5120/13374] loss=0.0001850948017818155\n",
      "[0m 51s] Epoch 36 [7680/13374] loss=0.00019198407171643338\n",
      "[0m 52s] Epoch 36 [10240/13374] loss=0.00020011909455206479\n",
      "[0m 52s] Epoch 36 [12800/13374] loss=0.00021332338816137052\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5641/6700 84.19%\n",
      "[0m 52s] Epoch 37 [2560/13374] loss=0.00016167235226021147\n",
      "[0m 53s] Epoch 37 [5120/13374] loss=0.00017194103202200495\n",
      "[0m 53s] Epoch 37 [7680/13374] loss=0.00018426595973627022\n",
      "[0m 53s] Epoch 37 [10240/13374] loss=0.00019440134055912495\n",
      "[0m 53s] Epoch 37 [12800/13374] loss=0.00020985977331292816\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5627/6700 83.99%\n",
      "[0m 54s] Epoch 38 [2560/13374] loss=0.00016807710999273696\n",
      "[0m 54s] Epoch 38 [5120/13374] loss=0.00016607105935690926\n",
      "[0m 54s] Epoch 38 [7680/13374] loss=0.00018030789409143229\n",
      "[0m 55s] Epoch 38 [10240/13374] loss=0.00019041263331018854\n",
      "[0m 55s] Epoch 38 [12800/13374] loss=0.00020192893251078203\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5612/6700 83.76%\n",
      "[0m 56s] Epoch 39 [2560/13374] loss=0.00016699730986147188\n",
      "[0m 56s] Epoch 39 [5120/13374] loss=0.00016951363541011232\n",
      "[0m 56s] Epoch 39 [7680/13374] loss=0.00017625749145130007\n",
      "[0m 56s] Epoch 39 [10240/13374] loss=0.0001857357707194751\n",
      "[0m 57s] Epoch 39 [12800/13374] loss=0.00020370928599731995\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5603/6700 83.63%\n",
      "[0m 57s] Epoch 40 [2560/13374] loss=0.00014633174178015906\n",
      "[0m 57s] Epoch 40 [5120/13374] loss=0.00016809725257189713\n",
      "[0m 58s] Epoch 40 [7680/13374] loss=0.000171085766608788\n",
      "[0m 58s] Epoch 40 [10240/13374] loss=0.000184825408996403\n",
      "[0m 58s] Epoch 40 [12800/13374] loss=0.0001964159806811949\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5626/6700 83.97%\n",
      "[0m 59s] Epoch 41 [2560/13374] loss=0.00015390960397780872\n",
      "[0m 59s] Epoch 41 [5120/13374] loss=0.00015831680357223377\n",
      "[0m 59s] Epoch 41 [7680/13374] loss=0.000172872874342526\n",
      "[0m 59s] Epoch 41 [10240/13374] loss=0.00018757994439511094\n",
      "[1m 0s] Epoch 41 [12800/13374] loss=0.00020148481999058277\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5620/6700 83.88%\n",
      "[1m 0s] Epoch 42 [2560/13374] loss=0.00013967685808893293\n",
      "[1m 0s] Epoch 42 [5120/13374] loss=0.00015364123864856083\n",
      "[1m 1s] Epoch 42 [7680/13374] loss=0.00017564116399929237\n",
      "[1m 1s] Epoch 42 [10240/13374] loss=0.00017887226968014148\n",
      "[1m 1s] Epoch 42 [12800/13374] loss=0.00019066911336267366\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5629/6700 84.01%\n",
      "[1m 2s] Epoch 43 [2560/13374] loss=0.00014248515799408779\n",
      "[1m 2s] Epoch 43 [5120/13374] loss=0.00015819675718375947\n",
      "[1m 2s] Epoch 43 [7680/13374] loss=0.00018009696917336743\n",
      "[1m 2s] Epoch 43 [10240/13374] loss=0.00020015184363728623\n",
      "[1m 3s] Epoch 43 [12800/13374] loss=0.0001956530351890251\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5609/6700 83.72%\n",
      "[1m 3s] Epoch 44 [2560/13374] loss=0.00016188604204216971\n",
      "[1m 4s] Epoch 44 [5120/13374] loss=0.00016486860076838638\n",
      "[1m 4s] Epoch 44 [7680/13374] loss=0.0001759454489122921\n",
      "[1m 4s] Epoch 44 [10240/13374] loss=0.0001836839799580048\n",
      "[1m 4s] Epoch 44 [12800/13374] loss=0.00019444619771093131\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5614/6700 83.79%\n",
      "[1m 5s] Epoch 45 [2560/13374] loss=0.00018059921567328274\n",
      "[1m 5s] Epoch 45 [5120/13374] loss=0.00017400120486854576\n",
      "[1m 5s] Epoch 45 [7680/13374] loss=0.00016920708779556056\n",
      "[1m 6s] Epoch 45 [10240/13374] loss=0.00018183153697464148\n",
      "[1m 6s] Epoch 45 [12800/13374] loss=0.0001950013660825789\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5620/6700 83.88%\n",
      "[1m 7s] Epoch 46 [2560/13374] loss=0.00015120170428417623\n",
      "[1m 7s] Epoch 46 [5120/13374] loss=0.00015846639362280258\n",
      "[1m 7s] Epoch 46 [7680/13374] loss=0.0001707669405732304\n",
      "[1m 7s] Epoch 46 [10240/13374] loss=0.0001770576791386702\n",
      "[1m 8s] Epoch 46 [12800/13374] loss=0.00018274225061759352\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5619/6700 83.87%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1m 8s] Epoch 47 [2560/13374] loss=0.0001455776593502378\n",
      "[1m 8s] Epoch 47 [5120/13374] loss=0.00018071197355311596\n",
      "[1m 9s] Epoch 47 [7680/13374] loss=0.0001766626137396088\n",
      "[1m 9s] Epoch 47 [10240/13374] loss=0.00018386001074759405\n",
      "[1m 9s] Epoch 47 [12800/13374] loss=0.00018741095533187036\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5613/6700 83.78%\n",
      "[1m 10s] Epoch 48 [2560/13374] loss=0.00013768539793090895\n",
      "[1m 10s] Epoch 48 [5120/13374] loss=0.00014304139513114932\n",
      "[1m 10s] Epoch 48 [7680/13374] loss=0.00015800136388861575\n",
      "[1m 11s] Epoch 48 [10240/13374] loss=0.0001617371321117389\n",
      "[1m 11s] Epoch 48 [12800/13374] loss=0.0001782253752753604\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5623/6700 83.93%\n",
      "[1m 11s] Epoch 49 [2560/13374] loss=0.00012643772170122246\n",
      "[1m 12s] Epoch 49 [5120/13374] loss=0.00014895947879267624\n",
      "[1m 12s] Epoch 49 [7680/13374] loss=0.00017168260516579417\n",
      "[1m 12s] Epoch 49 [10240/13374] loss=0.00016821121143948404\n",
      "[1m 12s] Epoch 49 [12800/13374] loss=0.00018054186330118682\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5624/6700 83.94%\n",
      "[1m 13s] Epoch 50 [2560/13374] loss=0.0001368809687846806\n",
      "[1m 13s] Epoch 50 [5120/13374] loss=0.00014242696779547258\n",
      "[1m 14s] Epoch 50 [7680/13374] loss=0.0001507612891145982\n",
      "[1m 14s] Epoch 50 [10240/13374] loss=0.00017153369190054947\n",
      "[1m 14s] Epoch 50 [12800/13374] loss=0.00017929480789462106\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5620/6700 83.88%\n",
      "[1m 15s] Epoch 51 [2560/13374] loss=0.00014369915661518461\n",
      "[1m 15s] Epoch 51 [5120/13374] loss=0.00016301044306601397\n",
      "[1m 15s] Epoch 51 [7680/13374] loss=0.00016461907507618888\n",
      "[1m 16s] Epoch 51 [10240/13374] loss=0.0001663034374359995\n",
      "[1m 16s] Epoch 51 [12800/13374] loss=0.000178282453562133\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5632/6700 84.06%\n",
      "[1m 16s] Epoch 52 [2560/13374] loss=0.00015530551900155842\n",
      "[1m 17s] Epoch 52 [5120/13374] loss=0.00015068727516336365\n",
      "[1m 17s] Epoch 52 [7680/13374] loss=0.00016448328678961844\n",
      "[1m 17s] Epoch 52 [10240/13374] loss=0.00016206205655180383\n",
      "[1m 17s] Epoch 52 [12800/13374] loss=0.0001720081883831881\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5589/6700 83.42%\n",
      "[1m 18s] Epoch 53 [2560/13374] loss=0.00015714801411377267\n",
      "[1m 18s] Epoch 53 [5120/13374] loss=0.00015880781247687992\n",
      "[1m 19s] Epoch 53 [7680/13374] loss=0.00016015612563933246\n",
      "[1m 19s] Epoch 53 [10240/13374] loss=0.00016400910972151904\n",
      "[1m 19s] Epoch 53 [12800/13374] loss=0.00017487908087787218\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5618/6700 83.85%\n",
      "[1m 20s] Epoch 54 [2560/13374] loss=0.00014836026821285486\n",
      "[1m 20s] Epoch 54 [5120/13374] loss=0.00014602701558033004\n",
      "[1m 20s] Epoch 54 [7680/13374] loss=0.00015865715880257388\n",
      "[1m 20s] Epoch 54 [10240/13374] loss=0.00016989490868581924\n",
      "[1m 20s] Epoch 54 [12800/13374] loss=0.00017465567856561392\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5612/6700 83.76%\n",
      "[1m 21s] Epoch 55 [2560/13374] loss=0.0001503711158875376\n",
      "[1m 21s] Epoch 55 [5120/13374] loss=0.00015963901569193695\n",
      "[1m 21s] Epoch 55 [7680/13374] loss=0.00016304421345315253\n",
      "[1m 22s] Epoch 55 [10240/13374] loss=0.00016677692347002448\n",
      "[1m 22s] Epoch 55 [12800/13374] loss=0.00017137701390311123\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5612/6700 83.76%\n",
      "[1m 22s] Epoch 56 [2560/13374] loss=0.00012594134095706978\n",
      "[1m 22s] Epoch 56 [5120/13374] loss=0.00013734805834246798\n",
      "[1m 23s] Epoch 56 [7680/13374] loss=0.00014516131462490497\n",
      "[1m 23s] Epoch 56 [10240/13374] loss=0.00015999334009393352\n",
      "[1m 23s] Epoch 56 [12800/13374] loss=0.00017287410053540953\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5619/6700 83.87%\n",
      "[1m 24s] Epoch 57 [2560/13374] loss=0.0001674936356721446\n",
      "[1m 24s] Epoch 57 [5120/13374] loss=0.00014460227812378433\n",
      "[1m 24s] Epoch 57 [7680/13374] loss=0.0001588627127299939\n",
      "[1m 24s] Epoch 57 [10240/13374] loss=0.00016220172410612576\n",
      "[1m 24s] Epoch 57 [12800/13374] loss=0.00016610104466963094\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5627/6700 83.99%\n",
      "[1m 25s] Epoch 58 [2560/13374] loss=0.0001360978523734957\n",
      "[1m 25s] Epoch 58 [5120/13374] loss=0.00015432820364367217\n",
      "[1m 26s] Epoch 58 [7680/13374] loss=0.00014853070460958407\n",
      "[1m 26s] Epoch 58 [10240/13374] loss=0.00015758736590214538\n",
      "[1m 26s] Epoch 58 [12800/13374] loss=0.00017100911238230766\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5604/6700 83.64%\n",
      "[1m 28s] Epoch 59 [2560/13374] loss=0.0001360245209070854\n",
      "[1m 28s] Epoch 59 [5120/13374] loss=0.0001616647896298673\n",
      "[1m 28s] Epoch 59 [7680/13374] loss=0.00016428303739909704\n",
      "[1m 29s] Epoch 59 [10240/13374] loss=0.00016551960143260658\n",
      "[1m 29s] Epoch 59 [12800/13374] loss=0.00017316138691967354\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5614/6700 83.79%\n",
      "[1m 29s] Epoch 60 [2560/13374] loss=0.00015588309906888753\n",
      "[1m 30s] Epoch 60 [5120/13374] loss=0.00014409931645786855\n",
      "[1m 30s] Epoch 60 [7680/13374] loss=0.0001542427084738544\n",
      "[1m 30s] Epoch 60 [10240/13374] loss=0.00015762730799906421\n",
      "[1m 30s] Epoch 60 [12800/13374] loss=0.00016568670689594002\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5607/6700 83.69%\n",
      "[1m 31s] Epoch 61 [2560/13374] loss=0.0001448867864382919\n",
      "[1m 31s] Epoch 61 [5120/13374] loss=0.00015185825832304546\n",
      "[1m 31s] Epoch 61 [7680/13374] loss=0.0001535328075988218\n",
      "[1m 31s] Epoch 61 [10240/13374] loss=0.00015725275934528327\n",
      "[1m 32s] Epoch 61 [12800/13374] loss=0.0001633098530874122\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5612/6700 83.76%\n",
      "[1m 32s] Epoch 62 [2560/13374] loss=0.00013238021783763544\n",
      "[1m 32s] Epoch 62 [5120/13374] loss=0.00013208493146521505\n",
      "[1m 33s] Epoch 62 [7680/13374] loss=0.00014157858361916927\n",
      "[1m 33s] Epoch 62 [10240/13374] loss=0.00015191234997473656\n",
      "[1m 33s] Epoch 62 [12800/13374] loss=0.00016266302729491143\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5639/6700 84.16%\n",
      "[1m 34s] Epoch 63 [2560/13374] loss=0.0001503587445768062\n",
      "[1m 34s] Epoch 63 [5120/13374] loss=0.00014321671515062916\n",
      "[1m 34s] Epoch 63 [7680/13374] loss=0.0001588383946606579\n",
      "[1m 34s] Epoch 63 [10240/13374] loss=0.00015947508109093178\n",
      "[1m 35s] Epoch 63 [12800/13374] loss=0.00016394429359934293\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5614/6700 83.79%\n",
      "[1m 35s] Epoch 64 [2560/13374] loss=0.00014180861180648207\n",
      "[1m 35s] Epoch 64 [5120/13374] loss=0.0001472329724492738\n",
      "[1m 36s] Epoch 64 [7680/13374] loss=0.0001489686483788925\n",
      "[1m 36s] Epoch 64 [10240/13374] loss=0.00015931248672131914\n",
      "[1m 36s] Epoch 64 [12800/13374] loss=0.00016903626354178414\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5608/6700 83.70%\n",
      "[1m 37s] Epoch 65 [2560/13374] loss=0.00014142570507829076\n",
      "[1m 37s] Epoch 65 [5120/13374] loss=0.00014180277576087975\n",
      "[1m 37s] Epoch 65 [7680/13374] loss=0.00015173519908178907\n",
      "[1m 37s] Epoch 65 [10240/13374] loss=0.0001512353388534393\n",
      "[1m 37s] Epoch 65 [12800/13374] loss=0.00016174679039977492\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5607/6700 83.69%\n",
      "[1m 38s] Epoch 66 [2560/13374] loss=0.00015511956662521696\n",
      "[1m 38s] Epoch 66 [5120/13374] loss=0.0001464279368519783\n",
      "[1m 38s] Epoch 66 [7680/13374] loss=0.0001520597046085944\n",
      "[1m 39s] Epoch 66 [10240/13374] loss=0.00015569969673379092\n",
      "[1m 39s] Epoch 66 [12800/13374] loss=0.0001587846441543661\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5599/6700 83.57%\n",
      "[1m 39s] Epoch 67 [2560/13374] loss=0.00012191048081149347\n",
      "[1m 40s] Epoch 67 [5120/13374] loss=0.00014509680077026132\n",
      "[1m 40s] Epoch 67 [7680/13374] loss=0.0001546028557640966\n",
      "[1m 40s] Epoch 67 [10240/13374] loss=0.00015846149417484413\n",
      "[1m 40s] Epoch 67 [12800/13374] loss=0.0001665187711478211\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5611/6700 83.75%\n",
      "[1m 41s] Epoch 68 [2560/13374] loss=0.0001321456460573245\n",
      "[1m 41s] Epoch 68 [5120/13374] loss=0.00013795085651509\n",
      "[1m 41s] Epoch 68 [7680/13374] loss=0.00014799735229947448\n",
      "[1m 42s] Epoch 68 [10240/13374] loss=0.00015116415752345347\n",
      "[1m 42s] Epoch 68 [12800/13374] loss=0.0001610577054816531\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5623/6700 83.93%\n",
      "[1m 42s] Epoch 69 [2560/13374] loss=0.0001025045563437743\n",
      "[1m 43s] Epoch 69 [5120/13374] loss=0.00013700728832191089\n",
      "[1m 43s] Epoch 69 [7680/13374] loss=0.00014438041774459027\n",
      "[1m 43s] Epoch 69 [10240/13374] loss=0.0001553447625155968\n",
      "[1m 43s] Epoch 69 [12800/13374] loss=0.0001615623075485928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating trained model ...\n",
      "Test set: Accuracy 5607/6700 83.69%\n",
      "[1m 44s] Epoch 70 [2560/13374] loss=0.0001579105075506959\n",
      "[1m 44s] Epoch 70 [5120/13374] loss=0.00015237415027513634\n",
      "[1m 44s] Epoch 70 [7680/13374] loss=0.00015963900029116\n",
      "[1m 45s] Epoch 70 [10240/13374] loss=0.0001629738719202578\n",
      "[1m 45s] Epoch 70 [12800/13374] loss=0.00016322483381372876\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5621/6700 83.90%\n",
      "[1m 45s] Epoch 71 [2560/13374] loss=0.00012135722281527705\n",
      "[1m 45s] Epoch 71 [5120/13374] loss=0.00011754556671803585\n",
      "[1m 46s] Epoch 71 [7680/13374] loss=0.000128829839983761\n",
      "[1m 46s] Epoch 71 [10240/13374] loss=0.00014621609579990036\n",
      "[1m 46s] Epoch 71 [12800/13374] loss=0.00015729088940133807\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5611/6700 83.75%\n",
      "[1m 47s] Epoch 72 [2560/13374] loss=0.00013543285713240038\n",
      "[1m 47s] Epoch 72 [5120/13374] loss=0.00015564035329589386\n",
      "[1m 47s] Epoch 72 [7680/13374] loss=0.00015216206629702355\n",
      "[1m 47s] Epoch 72 [10240/13374] loss=0.00015323162342610884\n",
      "[1m 47s] Epoch 72 [12800/13374] loss=0.00016110685442981775\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5622/6700 83.91%\n",
      "[1m 48s] Epoch 73 [2560/13374] loss=0.0001280820906686131\n",
      "[1m 48s] Epoch 73 [5120/13374] loss=0.00013388177576416637\n",
      "[1m 48s] Epoch 73 [7680/13374] loss=0.00014456644906507183\n",
      "[1m 49s] Epoch 73 [10240/13374] loss=0.00015318398609451834\n",
      "[1m 49s] Epoch 73 [12800/13374] loss=0.00015941506448143627\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5624/6700 83.94%\n",
      "[1m 49s] Epoch 74 [2560/13374] loss=0.0001204170381242875\n",
      "[1m 49s] Epoch 74 [5120/13374] loss=0.0001335821121756453\n",
      "[1m 50s] Epoch 74 [7680/13374] loss=0.00013566638566165541\n",
      "[1m 50s] Epoch 74 [10240/13374] loss=0.00014910768695699516\n",
      "[1m 50s] Epoch 74 [12800/13374] loss=0.0001576423627557233\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5609/6700 83.72%\n",
      "[1m 51s] Epoch 75 [2560/13374] loss=0.0001415575337887276\n",
      "[1m 51s] Epoch 75 [5120/13374] loss=0.00013771080375590827\n",
      "[1m 51s] Epoch 75 [7680/13374] loss=0.00013525760368793271\n",
      "[1m 51s] Epoch 75 [10240/13374] loss=0.0001455485456972383\n",
      "[1m 52s] Epoch 75 [12800/13374] loss=0.00015495086117880418\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5616/6700 83.82%\n",
      "[1m 52s] Epoch 76 [2560/13374] loss=0.00011875904092448764\n",
      "[1m 53s] Epoch 76 [5120/13374] loss=0.0001306967962591443\n",
      "[1m 53s] Epoch 76 [7680/13374] loss=0.00012842595533584243\n",
      "[1m 53s] Epoch 76 [10240/13374] loss=0.0001499334843174438\n",
      "[1m 53s] Epoch 76 [12800/13374] loss=0.0001559230074781226\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5627/6700 83.99%\n",
      "[1m 54s] Epoch 77 [2560/13374] loss=0.00013027676250203512\n",
      "[1m 54s] Epoch 77 [5120/13374] loss=0.0001393021007970674\n",
      "[1m 54s] Epoch 77 [7680/13374] loss=0.00014349477726985544\n",
      "[1m 54s] Epoch 77 [10240/13374] loss=0.0001460131915337115\n",
      "[1m 55s] Epoch 77 [12800/13374] loss=0.00015427339581947308\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5604/6700 83.64%\n",
      "[1m 55s] Epoch 78 [2560/13374] loss=0.00012678494640567807\n",
      "[1m 55s] Epoch 78 [5120/13374] loss=0.0001304976320170681\n",
      "[1m 56s] Epoch 78 [7680/13374] loss=0.00013308032054434686\n",
      "[1m 56s] Epoch 78 [10240/13374] loss=0.0001492648373641714\n",
      "[1m 56s] Epoch 78 [12800/13374] loss=0.00015331727532611695\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5617/6700 83.84%\n",
      "[1m 57s] Epoch 79 [2560/13374] loss=0.00014174709867802447\n",
      "[1m 57s] Epoch 79 [5120/13374] loss=0.00014649697022832696\n",
      "[1m 57s] Epoch 79 [7680/13374] loss=0.0001483818182653825\n",
      "[1m 57s] Epoch 79 [10240/13374] loss=0.0001439115733774088\n",
      "[1m 58s] Epoch 79 [12800/13374] loss=0.00014747679546417203\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5614/6700 83.79%\n",
      "[1m 58s] Epoch 80 [2560/13374] loss=0.00015825779919396155\n",
      "[1m 58s] Epoch 80 [5120/13374] loss=0.00014072069971007294\n",
      "[1m 59s] Epoch 80 [7680/13374] loss=0.0001460313673305791\n",
      "[1m 59s] Epoch 80 [10240/13374] loss=0.00015341287180490327\n",
      "[1m 59s] Epoch 80 [12800/13374] loss=0.00015513387799728662\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5621/6700 83.90%\n",
      "[2m 0s] Epoch 81 [2560/13374] loss=0.00013700365598197096\n",
      "[2m 0s] Epoch 81 [5120/13374] loss=0.00012566520181280793\n",
      "[2m 0s] Epoch 81 [7680/13374] loss=0.00013781616811077887\n",
      "[2m 0s] Epoch 81 [10240/13374] loss=0.00013553170165323536\n",
      "[2m 0s] Epoch 81 [12800/13374] loss=0.00014937713283870834\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5602/6700 83.61%\n",
      "[2m 1s] Epoch 82 [2560/13374] loss=0.00013141398376319556\n",
      "[2m 1s] Epoch 82 [5120/13374] loss=0.000137257406822755\n",
      "[2m 1s] Epoch 82 [7680/13374] loss=0.00013920149237189132\n",
      "[2m 2s] Epoch 82 [10240/13374] loss=0.00014800271765125217\n",
      "[2m 2s] Epoch 82 [12800/13374] loss=0.00015322210572776384\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5604/6700 83.64%\n",
      "[2m 2s] Epoch 83 [2560/13374] loss=0.0001433871490007732\n",
      "[2m 3s] Epoch 83 [5120/13374] loss=0.00014111437885730994\n",
      "[2m 3s] Epoch 83 [7680/13374] loss=0.0001461529820517171\n",
      "[2m 3s] Epoch 83 [10240/13374] loss=0.0001487013701989781\n",
      "[2m 3s] Epoch 83 [12800/13374] loss=0.00015212669066386297\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5610/6700 83.73%\n",
      "[2m 4s] Epoch 84 [2560/13374] loss=0.00013588790607172997\n",
      "[2m 4s] Epoch 84 [5120/13374] loss=0.0001325675058978959\n",
      "[2m 4s] Epoch 84 [7680/13374] loss=0.0001471644967144433\n",
      "[2m 5s] Epoch 84 [10240/13374] loss=0.00015142101592573453\n",
      "[2m 5s] Epoch 84 [12800/13374] loss=0.00015095143688085956\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5607/6700 83.69%\n",
      "[2m 6s] Epoch 85 [2560/13374] loss=0.00011051117544411682\n",
      "[2m 6s] Epoch 85 [5120/13374] loss=0.00012572226987686008\n",
      "[2m 6s] Epoch 85 [7680/13374] loss=0.0001323698847651637\n",
      "[2m 6s] Epoch 85 [10240/13374] loss=0.0001396063027641503\n",
      "[2m 7s] Epoch 85 [12800/13374] loss=0.0001482456419034861\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5615/6700 83.81%\n",
      "[2m 7s] Epoch 86 [2560/13374] loss=0.0001287765728193335\n",
      "[2m 7s] Epoch 86 [5120/13374] loss=0.00013336746997083537\n",
      "[2m 8s] Epoch 86 [7680/13374] loss=0.00014264795245253482\n",
      "[2m 8s] Epoch 86 [10240/13374] loss=0.00014703693486808333\n",
      "[2m 8s] Epoch 86 [12800/13374] loss=0.0001524890925793443\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5592/6700 83.46%\n",
      "[2m 9s] Epoch 87 [2560/13374] loss=0.00013406254875008016\n",
      "[2m 9s] Epoch 87 [5120/13374] loss=0.00013709006507269805\n",
      "[2m 9s] Epoch 87 [7680/13374] loss=0.00013503358435021557\n",
      "[2m 10s] Epoch 87 [10240/13374] loss=0.00013829327062921947\n",
      "[2m 10s] Epoch 87 [12800/13374] loss=0.00014553355889802334\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5604/6700 83.64%\n",
      "[2m 10s] Epoch 88 [2560/13374] loss=0.00011227366994717158\n",
      "[2m 11s] Epoch 88 [5120/13374] loss=0.00014272721273300704\n",
      "[2m 11s] Epoch 88 [7680/13374] loss=0.0001438161457675354\n",
      "[2m 11s] Epoch 88 [10240/13374] loss=0.00014508950152958278\n",
      "[2m 11s] Epoch 88 [12800/13374] loss=0.00015193720217212102\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5607/6700 83.69%\n",
      "[2m 12s] Epoch 89 [2560/13374] loss=0.00011714126958395354\n",
      "[2m 12s] Epoch 89 [5120/13374] loss=0.0001250750876351958\n",
      "[2m 13s] Epoch 89 [7680/13374] loss=0.0001346611781627871\n",
      "[2m 13s] Epoch 89 [10240/13374] loss=0.00014107401402725374\n",
      "[2m 13s] Epoch 89 [12800/13374] loss=0.00015072649723151698\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5610/6700 83.73%\n",
      "[2m 14s] Epoch 90 [2560/13374] loss=0.00012858880072599276\n",
      "[2m 14s] Epoch 90 [5120/13374] loss=0.00013357391489989822\n",
      "[2m 14s] Epoch 90 [7680/13374] loss=0.00013684698242286686\n",
      "[2m 14s] Epoch 90 [10240/13374] loss=0.00014037303235454602\n",
      "[2m 15s] Epoch 90 [12800/13374] loss=0.00014681970780657137\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5612/6700 83.76%\n",
      "[2m 15s] Epoch 91 [2560/13374] loss=0.00012991408912057522\n",
      "[2m 15s] Epoch 91 [5120/13374] loss=0.00012065187856933335\n",
      "[2m 16s] Epoch 91 [7680/13374] loss=0.0001279299038287718\n",
      "[2m 16s] Epoch 91 [10240/13374] loss=0.00014455874752457022\n",
      "[2m 16s] Epoch 91 [12800/13374] loss=0.00015066820473293773\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5612/6700 83.76%\n",
      "[2m 17s] Epoch 92 [2560/13374] loss=0.00011729561592801473\n",
      "[2m 17s] Epoch 92 [5120/13374] loss=0.00014075182625674642\n",
      "[2m 17s] Epoch 92 [7680/13374] loss=0.00014254277678749834\n",
      "[2m 17s] Epoch 92 [10240/13374] loss=0.00014544634977937677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2m 18s] Epoch 92 [12800/13374] loss=0.00015377625633846036\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5599/6700 83.57%\n",
      "[2m 18s] Epoch 93 [2560/13374] loss=0.00013782445530523545\n",
      "[2m 18s] Epoch 93 [5120/13374] loss=0.0001293986730161123\n",
      "[2m 19s] Epoch 93 [7680/13374] loss=0.00013990123164451992\n",
      "[2m 19s] Epoch 93 [10240/13374] loss=0.0001470263379815151\n",
      "[2m 19s] Epoch 93 [12800/13374] loss=0.0001505094971798826\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5620/6700 83.88%\n",
      "[2m 20s] Epoch 94 [2560/13374] loss=0.00011785887800215278\n",
      "[2m 20s] Epoch 94 [5120/13374] loss=0.00013497970594471552\n",
      "[2m 20s] Epoch 94 [7680/13374] loss=0.0001362246937787859\n",
      "[2m 20s] Epoch 94 [10240/13374] loss=0.00014380891125256312\n",
      "[2m 21s] Epoch 94 [12800/13374] loss=0.00014793479967920576\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5602/6700 83.61%\n",
      "[2m 21s] Epoch 95 [2560/13374] loss=0.0001380400950438343\n",
      "[2m 21s] Epoch 95 [5120/13374] loss=0.00013434712564048824\n",
      "[2m 22s] Epoch 95 [7680/13374] loss=0.00013948084015282802\n",
      "[2m 22s] Epoch 95 [10240/13374] loss=0.0001421758664037043\n",
      "[2m 22s] Epoch 95 [12800/13374] loss=0.0001459886413795175\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5612/6700 83.76%\n",
      "[2m 23s] Epoch 96 [2560/13374] loss=0.0001362812145089265\n",
      "[2m 23s] Epoch 96 [5120/13374] loss=0.00013535624639189338\n",
      "[2m 23s] Epoch 96 [7680/13374] loss=0.0001412765416413701\n",
      "[2m 24s] Epoch 96 [10240/13374] loss=0.000139706520440086\n",
      "[2m 24s] Epoch 96 [12800/13374] loss=0.0001481640860583866\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5612/6700 83.76%\n",
      "[2m 24s] Epoch 97 [2560/13374] loss=0.00013184862255002371\n",
      "[2m 25s] Epoch 97 [5120/13374] loss=0.00011944958478125046\n",
      "[2m 25s] Epoch 97 [7680/13374] loss=0.00013513940320990513\n",
      "[2m 25s] Epoch 97 [10240/13374] loss=0.00013989706758366083\n",
      "[2m 25s] Epoch 97 [12800/13374] loss=0.00014990058138209862\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5597/6700 83.54%\n",
      "[2m 26s] Epoch 98 [2560/13374] loss=0.00038015426034689883\n",
      "[2m 26s] Epoch 98 [5120/13374] loss=0.0005004382608603918\n",
      "[2m 27s] Epoch 98 [7680/13374] loss=0.0005126235165031783\n",
      "[2m 27s] Epoch 98 [10240/13374] loss=0.0005058555647337925\n",
      "[2m 27s] Epoch 98 [12800/13374] loss=0.0004996390840096864\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5542/6700 82.72%\n",
      "[2m 28s] Epoch 99 [2560/13374] loss=0.0003279545097029768\n",
      "[2m 28s] Epoch 99 [5120/13374] loss=0.0003078710928093642\n",
      "[2m 28s] Epoch 99 [7680/13374] loss=0.00031481239275308324\n",
      "[2m 29s] Epoch 99 [10240/13374] loss=0.0003188663675246062\n",
      "[2m 29s] Epoch 99 [12800/13374] loss=0.00030326534193591214\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5573/6700 83.18%\n",
      "[2m 29s] Epoch 100 [2560/13374] loss=0.00017300968975177966\n",
      "[2m 30s] Epoch 100 [5120/13374] loss=0.00018482008163118736\n",
      "[2m 30s] Epoch 100 [7680/13374] loss=0.00018364118611013207\n",
      "[2m 30s] Epoch 100 [10240/13374] loss=0.00019626252997113626\n",
      "[2m 30s] Epoch 100 [12800/13374] loss=0.00020299999188864605\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5620/6700 83.88%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)\n",
    "    if USE_GPU:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        classifier.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    start = time.time()\n",
    "    print(\"Training for %d epochs...\" % N_EPOCHS)\n",
    "    acc_list = []\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        # Train cycle\n",
    "        trainModel()\n",
    "        acc = testModel()\n",
    "        acc_list.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5fcf641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAAGwCAYAAABSN5pGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABlYklEQVR4nO3deVxU5f4H8M9sDAyyI6soaIZLuIEgaeaKllmmLS4pmmmLlMm9v1JTSVvo2s28lmkLLjc1zK6mpZmIqZkoCu4L7qKyCCEMiwwDc35/AKPDIoszcwb8vF8vXjXnnDnznO+o58vzfJ/nSARBEEBEREREelKxG0BERERkaZggEREREVXBBImIiIioCiZIRERERFUwQSIiIiKqggkSERERURVMkIiIiIiqkIvdgKZKp9MhLS0NdnZ2kEgkYjeHiIiI6kEQBOTn58PLywtSae39REyQGiktLQ0+Pj5iN4OIiIga4dq1a2jVqlWt+5kgNZKdnR2A8gDb29s3+jxarRY7duxAWFgYFAqFsZpHNWCszYexNh/G2nwYa/MxZazVajV8fHz09/HaMEFqpMphNXt7+/tOkFQqFezt7fkXzsQYa/NhrM2HsTYfxtp8zBHruspjWKRNREREVAUTJCIiIqIqRE+Qli5dCl9fX1hbWyMkJASJiYn3PH7x4sXw9/eHjY0NfHx8MGPGDBQXF+v3v//++5BIJAY/HTp0MDhHcXExpk2bBhcXF7Ro0QKjRo1CZmamSa6PiIiImh5RE6T169cjMjISUVFRSE5ORteuXTFkyBDcvHmzxuPXrVuHmTNnIioqCmfOnEFMTAzWr1+P2bNnGxzXuXNnpKen63/27dtnsH/GjBn45ZdfsGHDBuzZswdpaWkYOXKkya6TiIiImhZRi7QXLVqEKVOmYNKkSQCA5cuXY+vWrVixYgVmzpxZ7fj9+/ejd+/eGDt2LADA19cXY8aMwcGDBw2Ok8vl8PDwqPEz8/LyEBMTg3Xr1mHAgAEAgJUrV6Jjx444cOAAevXqVeP7NBoNNBqN/rVarQZQXkim1WobeOV3VL73fs5B9cNYmw9jbT6Mtfkw1uZjyljX95yiJUglJSVISkrCrFmz9NukUikGDRqEhISEGt/z6KOPYs2aNUhMTERwcDAuXbqEbdu2Yfz48QbHnT9/Hl5eXrC2tkZoaCiio6PRunVrAEBSUhK0Wi0GDRqkP75Dhw5o3bo1EhISak2QoqOjMX/+/Grbd+zYAZVK1eDrryouLu6+z0H1w1ibD2NtPoy1+TDW5mOKWBcVFdXrONESpOzsbJSVlcHd3d1gu7u7O86ePVvje8aOHYvs7Gz06dMHgiCgtLQUr732msEQW0hICFatWgV/f3+kp6dj/vz5eOyxx3Dy5EnY2dkhIyMDVlZWcHR0rPa5GRkZtbZ31qxZiIyM1L+uXEchLCzsvqf5x8XFYfDgwZw2amKMtfkw1ubDWJsPY20+pox15QhQXZrUOki7d+/Gxx9/jK+++gohISG4cOECpk+fjg8++ABz584FADzxxBP647t06YKQkBC0adMGP/74IyZPntzoz1YqlVAqldW2KxQKo3x5xjoP1Y2xNh/G2nwYa/NhrM3HFLGu7/lES5BcXV0hk8mqzR7LzMystX5o7ty5GD9+PF555RUAQEBAAAoLCzF16lS89957NT5TxdHREQ8//DAuXLgAAPDw8EBJSQlyc3MNepHu9blERET0YBFtFpuVlRUCAwMRHx+v36bT6RAfH4/Q0NAa31NUVFQtCZLJZADKHz5Xk4KCAly8eBGenp4AgMDAQCgUCoPPTUlJQWpqaq2fS0RERA8WUYfYIiMjER4ejqCgIAQHB2Px4sUoLCzUz2qbMGECvL29ER0dDQAYPnw4Fi1ahO7du+uH2ObOnYvhw4frE6V//vOfGD58ONq0aYO0tDRERUVBJpNhzJgxAAAHBwdMnjwZkZGRcHZ2hr29Pd58802EhobWWqBNREREDxZRE6QXX3wRWVlZmDdvHjIyMtCtWzds375dX7idmppq0GM0Z84cSCQSzJkzBzdu3EDLli0xfPhwfPTRR/pjrl+/jjFjxuDvv/9Gy5Yt0adPHxw4cAAtW7bUH/P5559DKpVi1KhR0Gg0GDJkCL766ivzXTgRERFZNNGLtCMiIhAREVHjvt27dxu8lsvliIqKQlRUVK3ni42NrfMzra2tsXTpUixdurRBbSXjK9SUwlYp+h9DIiIiA6I/aoQeTBezCvDK6sPoHPU73tt0AmW6mmvIiIiIxMBf3cmsbhWW4D/x57HmwFWUViRFaw+mIu+2Fote6AYrOXN2IiISHxMkMpvYxFR8vO0M1MWlAIBBHd3Q5yFXfLTtDH49no4CTSmWjQuEjZVM5JYSEdGDjgkSmcXpNDVmbjwBAOjoaY+5wzri0YdcAQC+rrZ4bU0SdqdkIXxFIr6bGAR7ay7CRkRE4uF4BpnFt39eAgCEdXLHr2/20SdHANDP3w3fTw6BnVKOxCs5GPPNAVzLqd+zcoiIiEyBCRKZ3I3c2/jlWBoA4M0B7SGTSqod09PXGT9M7QUXWyucSlNj2JI/8fup2p+NZwx7zmVh2tpkrNh3mQkZEREZYIJEJrdi32WU6gSEtnVBQCuHWo97xNsBmyN6o3trR6iLS/Hq90mY/8splJTqjNoeQRDw1e4LmLgyEVtPpGPBr6fx2MI/8OR//sTinedw/daDmyxduFmAopJSsZtBRCQ6JkhkUnm3tYhNTAUATH28bZ3Ht3JSYf3UUEx5zA8AsPKvK3h++X6k/t2wpKVMJ+BmfnG1R9CUlAEzfjyBhdtTIAjAE494IMTPGVIJcDpdjcU7z+OZL/+CuljboM9rDnaezsTgz/dg8KK9OJ+ZL3ZzyIhiE1MRtfkkirVlYjeFqMlgkTbdN51OwPEbefCwt4aHg7XBvrUHr6KwpAz+7nbo93DLWs5gyEouxXvDOiHEzwX/2HAMx67n4Yn/7MXcpzrhxZ4+kEiqD9FV9c5Px/G/5Otwt1eiV1sX9GrrgrYuNvjPKRmuF2ZALpXg/ac746VebQAAOYUliD+Tif/En8f1W7ex7mAqXnu8XcODUeHP81k4k67G5D5taxxStDSlZTpE/3YGglA+JDpy2X58Mz4Ioe1cxG4a3afTaWrM3nQCOgGwt1HgH2H+YjeJqElgDxI12rWcIvxn53n0+/dujFj6FwYv2oP4M5n6/ZrSMqz86woAYGrftvVKbO42qJM7tr7VB8G+zigsKcPMjScwefVh3FQX3/N9Z9LV+F/ydQBAplqDzUfTMGvjCbz4bSKuF0rgbKvA2ldC9MkRADjbWuH5IB9MH9geALDyr8uNHtrbey4Lk1YewsfbzurbYen+l3wdF7MK4aRSILCNE/KLSzFhxUFsPnpD7KaJplhbhsU7z+E/O8/X+jBsSycIAqK2nETlOqzLdl/E2Qy1uI0iaiKYIFGDHb+eizHfHMBjC//A5zvPITWnCFIJkK8pxSv/PYylf1yAIAj4+cgNZOVr4GFvjeFdvRr1Wa2cVPhhai+892RHWMmk2HX2JsIW78W2E+m1vmdJ/HkA5cNnP0zphekD2yPEzxlKuRR+dgI2vtYLIW1r7hl5pps33O2VFYlVw5ODkzfy8PqaJP0imF/uugBtmXFrqIytWFuGz+PKYzat/0NY+0oIngzwgLZMwPTYo/rv05jOZqgx5b+H8crqQyjUWF7N0+XsQoz8aj8W7zyPz3eew6Ert0z+mXm3tUi6esuosd58NA2HrtyCjUKGR9u5oFQn4N3/PXgr11/MKkBmHb9YEVXFITZqkFuFJXh51WFkF2ggkQC927niucBWGNDRDQu3n8WaA6n49PcUnE5X42x6+W+qL/fxva8VsmVSCab0bYvH/VtixvqjOJWmxhtrk/HVuB54MsDT4Ngz6Wr8djIDEgkwY/DDeNjdTj9MpNGUYPv23+DtaFPrZ1nJpXi5tx+ifzuLb/ZewqgerSCt5xDZtZwiTFx5CIUlZQht64LzN/ORmlOETUdu4IUgn2rHC4IA9e1SOKjMs+ZTblEJHGwU1XryVu+/ggx1MbwcrPFSrzawVsjw5Zge+MSpPAaf/p6CrHwN5j3VqV6xyC0qwXPL9iM7T4bT8vN4vmdrPOTWAgBwM78Yn8edw/pD1/S9Gv/30zEsHdujwT2MDVGmE3AuMx+3ikqgvl0KdbEW6ttauLSwQoifC7zu+jOx5VgaZv3vOApL7tTr/JR0DcF+ziZpW16RFjF/XcbKfZeRrynFjEEPY/qg9vd93gJNKT7edgYAEDHgITwX2AqDPtuDY9dysXr/Fbzcx+++P6MmgiAgu6AELe2URjmftkyHjLxiXLtVhOu3bkNlJcPQzh6Qy+r+N+V2SRn+tf0sVu2/AkeVAtun961WBkBUGyZI1CBRW04hu0CD9m4tsOrlYINk48MRAejk6YCoLSex9Xh5D4+dUo4xwa2N8tkPu9th0xu98f4vp7DuYCpm/u84urRyQCsnlf6Yyt6jYQGeeNjdzuD99U10xoS0xhe7LuD8zQLsPncTAzq41/menMIShK9IRHaBBh097fHNhEDEJl7DR9vO4Itd5/Fsd28o7voHvbRMhyn/PYx9F7Lx2Qvd8HQtPWzF2jIcSc1FYBunWpPMYm0Zjl/PQ2Abp1rrnbafzMAba5PQvbUTvhjTXZ8Q5N3W4qvdFwGUJ5TWivJVzKVSCWY/2RFeDtZ4/5fTWLX/CtTFWiwc1eWeNyZBEPDeppO4kFUIQIKv/7yMr/+8jO6tHdHdxwnrD6XqE48BHdzw5/ksbDuRga92X8S0/g/dO8gASkp1SLycg1KdDnbWCjjYyGFvrYCjyqrW+By/novZm07g5I3ah5ZaO6vQq60zSnUCNiaX9xwG+zpjXK/WmB57FNtOZGD+048YdZX3qolRpSW7zuNx/5bo5uN4X+f/Iv48buZr4OuiwiuP+UEpl2HWkx0xe9MJ/HtHCgZ3coePs6ruEzXArcIS/GPDMew6exPje7XB/Kc71/j3Lj3vNv6bcBUDOrihp2/NiefO05n4ZPtZXMoqQNUOr6GdPfCfMd2glNf+fRy9lovIH4/iUlYhACC3SIv/++kYVk8Krve/BXcr0JRiw6FUHLkhQViZDopmvpatprQMKRn5CPB2MOkvL7X5IyULJ3MkGFgqXqyZIFG9bT+ZgS3H0iCTSvDv57vW2BMzNqQ12ru3wOtrkpBdUIJxvdrAzoirYlvJpZj/dGecTlPj6LVcvB17FLFTe0Euk+J02p3eo7cGNv43cHtrBcaGtMY3ey/h6z2X7pkg3cwvxum08tlvl7IL4e1og1WTesLOWoFxvVrj670XcS3nNjYl38ALPe/0In3y21n8kZIFAHjnp2No19IWnb0Ml0Ao0JTipe8O4ui1XLR1tcXMJzpgcCd3/T9WOp2AX46n4V+/nUVaXjHCQ9tg/jOPVGtjaZkOC7efhU4Akq7ewpNL/sRnz3fFwI7u+HrPReTd1qK9WwuM7NGq2nsn9vaDg0qBf244jo3JN1BQXIolY7rrE6mqNh9Nw9YT6ZBJJXi6dSlyle7Ye/5vHEnNxZHUXABAVx9HzB3WEUG+zvghMRWzNpbfsDt52qN/B7dq5xQEAafS1Pgp6To2H72BW0XVZxhayaUY3MkdzwW2wmMPuUIukyK/WIvPdpzDfxOuQCcAKisZvB1tYG+jgL21HC2sFUjNKcKJ67lIzSlCasVaWBIJMK3fQ3h7UHtIJRJ8tqN8GPn3UxkY0d272mevOXAVBy/nYGhnDwzs6FZrbCpd/bsQq/ZfwYbD11FQkRj5u9th+qD22HYiHb8eT8eM9Uex9a0+UFk17p/oCzcLELPvMgAganhnfSIxuqcPfj56A4mXc/DezyexelJPo938kq7mIGLdEaTnlQ9lfX/gKnSCgA+eecQgITmdpsakVYnIVGuwbPdFvBjkg1lPdoCjygoAkF+sxYe/nsH6w9f077GSS9HKyQbejjY4eCkH209l4JXVh/H1+MBqMSop1eHLPy5g6R8XUKYT4G6vxFsD2+ODX0/jz/PZWJ1wBZN6V+89S7ycgws3C9DB0w4dPOz05039u6ji+7pWkcjK4PT7Ocx/JsAocbNEJaU6jPnmAJJTczG5jx/mPtXJ7G34z64LOJUmg8+RG5jwaN0zoE2BCRLVS05hCeb8XP6okFf7tkXXe/x229PXGdveegwJl/6uNgRmDAqZFF+M6Y4n//MnDl+9hSXx5xEZ5n/P3qOGmtTbFyv2XcbByzk4di1Xf71FJaVYdzAVe86Vz1LLLijRv8fBRoHVL/eEu315F77KSo5X+7Yr70X64zye7VHei7T56A18V3Hzeti9Bc5lFuDV75PwS0QfONmW3ySKtWWY+t/DOHotFwBwKbsQU79PQq+2zpgzrBOKtWX4YOsZHKvYDwBrDqZifGgbPORmeO2/HE/DpezyAmwfZxWOX8/D5NWHMSG0DX6suAn93xD/Wnufnu3eCi2UCkxbl4wdpzMxefUhfDM+CLZKw38+buTextzNJwEAEf3aou3tFDz5ZA/cKi7D5iNpOHo9F2Gd3DG8i5f+hjkmuDVO3MjDuoOpeCv2CLZE9IGfqy2A8rqRHacysfnoDZzNuLPsQEs7JdzslBXDZKXIL9aipFSHrcfTsfV4OlraKfHEIx7YcSoTGRV1JyO6eeG9YZ1qHPbJL9bi8NVbOHDpb1y/dRsvBvmg710zLkf28MbinefxU9L1agnS+cx8zNtcXgT9y7E0ONgo8HRXL4wKbIV2LW0Njj1xIw8r/7qCnWcyUVlm1MHDDtMHtseQzh6QSiXo3c4Vh6/cwuXsQny87Qw+HGF4Ez5+PRebjtxAP3839G3vWmNyIwgC3t9yCqU6AYM6uhkknVKpBNEjA/DEf/7E3nNZWBJ/ARN7+8LBpvG/xOh0Ar75s3wotkwnwM/VFqN6eOOzuHNYezAVAoAPK5Kkveey8MbaZBRoStHSTomsfA3WH76GnWcyMeepjvB0sME/NxzD9Vu3IZEAr/Txw5TH2sK1hVL/Z2bf+WxM/f4w/jyfjfExiVgR3hMOKgVuFZZgXWIqvk+4qv/en+7qhQXPdIajygplOgHzNp/CJ7+dRZ+HXNG+4t+IMp2Ahb+fxdd7LumvSSIB/Fxs0dJOicQrOfrvy8fJBtdu3cbqhFS0d7c3mOhxP/H7/VQGYvZdhp+rLf41qkujerjupVhbVq0W0lohM+jVvlv0b2eQXPELTcy+y/B0sMYrj9WcpNR0blsr+X1dw5XsQpxKy4cUAsI61d2DbyoSoalOzxCZWq2Gg4MD8vLyYG9v3+jzaLVabNu2DU8++SQUFtxn++YPR/DLsTQ87N4Cv7zZ555d2+ay5Vga3vrhCCQSYN5TnTD/l9OQSIDf3+5bY4LU0FhH/ngUG5NvYFiAJz59vgu+T7iKr/deQk7hnaRIKgHatmyBzl72mNq3bbVeoKKSUvRd+AeyC0qwcFQXdPKyx6hl+6Ep1WFa/3aY8lhbPP3lX0jNKULvh1ywelIwBACvr0nCzjM3YWslw7cTgvDXxWx8+2f5zDqJBPp/sG2tZHi9Xzskp+Zi19mbGNTRDd+F99R/fmmZDmGf78Wl7EK8M9Qfk/v4IXpbeU1GpR6tHfG/1x+tsydh/4VsvPLfwygqKcNDbi0w96lOeLwikdDpBIz97gAOXMpB99aOWPdyEHb8vr1esS4p1WHMtweQdPUW2ru1wKBO7thxKgMXK4ZGAMBKJsXgzoY9RJV0OgGn02vuYWrjosKHIx7BY+3rt8RETa7lFOGxhX9AIgH+eneAQb3S5FWHEH/2Jjp42CHvtlbfe1KXfv4t8XJvPzxWQ5Kz73w2Xoo5CABYOakn+vu7VesNA4CH3FpgUm9fDH/EHX/s/B2de/XDHyl/4/dTGTh89Ras5FLsnPE4WrtUH0Zb+scFfPp7CgBAKZdiSGcPjApshT4PuTZoWYqcwhL848ej+t7Qp7t64eORAWihlGPTkeuI/PEYBKE8Ee7m44DZm06irGLR2OXjA3EuMx/vbTqBc5kFBudt5WSDz57vWutkiuTUW5i4IhHq4lJ08LBD99ZO2HTkOoq15TfqlnZKRA3vhKe63Bm6FgQB4SsPYe+5LDzibY+Nr/fG7ZIyvBV7BHvOlbc/qI0TruYUIStfY/B5fR9uiZd7+yLU1xGR323H1msyyKQSrJ4UjD7tXXEvWfkaxJ3OhEImQScve7R3s4OVXAqdTsD2Uxn4z87zSLlr3bF5T3VqVH1YsbYM5zMLcDo9D5eyCnH91m193dbd/2ZVsreW4+ORAQYxAoBtJ9LxxtpkAMCwLp76coklY7oblAJk5Wvw4dbT2HIsDVWziHYtbfFdeE/9LzsN9eWu8/j3jnPo4KDDL/8cavR7Y33v30yQGulBSpC2n0zHa2uSIZNKsOmNR9GllaPYTdJ756dj+PHwnan0T3XxxJdje9R4bENjfTZDjaGL/4RUAjiqrPT/yLRxUWHio77o3toJ/u52ddalfLv3Ej7adgatnGz06wz182+JmPCekEklOJuhxsiv9qOopAyT+/ghu6B8aQKlXIrVLwejV8VN4vqtInz6ewo2H02DVAK82NMHMwY/DDc7a1zMKkDY53tRphMQO7WX/j0bk8tvUk4qBf58dwBaVPT6bD+Zjv/76TiKSsrww5Re9S5APnotF5NXHcLfFbF4/OGWeG9YR+w9l4UPt56BjUKG36Y/Bm8HqwbF+qa6GMO/3IdM9Z0bk0ImQWg7Vwzp7I6nArzqVcxeUqrDHyk3seNUJtq2tMXkPn51DnnVx4tfJ+Dg5Rz83xB/fa3U/ovZGPvtQcilEuyY0RdtXGyx/2I2fkq6jt9PZehv1pVUVjKM7OGNiY/66YvWazP/l1NY+dcVtLRT4t2hHfDv31P0vSKPtnPB8et5+uE5Bxs5bKBFxm3DxGbuU50wuZYbbZlOwH8TruCHxFSD5MTeWg5XOyXsrRX6ociQti54PrBVtTgevlI+pJahLtYPfY+usk7Z3UlSpWe7e+Nfo7roa8ZKSnX49s9LWBJ/HppSHUb39MGcpzrp/6zW5myGGuNjEg2Smc5e9pjcxw/DunjW+EtcproYQxbvRW6RFs8HtsLhq+W9ddYKKT59rqt+tu3N/GKcSS+fZNHLz1nf26TVarF16zb8cdsHm4+lw85ajk1v9K72fWpKy7DrzE38L/k6/kjJMpg1qJBJ8JCbHbRlOly4WR57O6Ucjz7kgt9PZcJKLsXWN/voP7MmOYUlOJOuxuk0NU5X/PdCVkGjZidO698O/xjsD6lUgktZBXj6y79QoCnFq4+3xcyhHTC/ogbRSibFqpd7opefC2IPXcMnv52Burj2GaiuLZT4fnIwOno2/P445PO9SMnMx5h2ZVgw8QkmSE1Nc02QzqSrsfV4OtTFWuTdLp/pk3T1FtTFpZjWvx3+b0gHsZtooKikFE99sQ+Xsgrv2XsENC7W4SsS9b9dtnFR4c0B7TGim1e9ZtBUul1ShscW7tIPx/m6qLA5oo/BsMbdv7UBgFwqwbcTgmqsyTmXmQ+FTFrtt7O5P5/E9weuoksrB/z8Rm/oBAGDP9+LyxW9R2/0MyyCziksQU5hSZ0366ryirT48o/zWLX/CrRlAqSS8pmG2jIBHz8bgLEhrRsV62PXyoup/VxtEdbZA/38W8LeiPVr9+PHw9fwzk/H0dbVFvH/eByCADy9dB9O3lBjQmgbLKhS+1WmE6oNOyhk0nr3zhRry/DUF/v0N1Cg/M/NBxW9YfnFWmw4fB2r9l/R107JpRKEtHVGWCcPDOrkfs/ZmpUEQcCJG3kVvW9pyLtd8wryHvbWeKN/O7wQ5AMrmRRf772Ef+8oH1Jr62qLL8f2QCevmv8dvDtJiuj/EP4R9nCNvZUZecXILtDgEe/aH0dU1ZXsQrzz03G4tLDCpN5+6OnrVGdPaNW/a96ONvh6fGC9Prfyz/XAsKGYuCoJh6/eQhsXFd4Z0gHpebdx/dZtXL9VhMNXbyH3rp7Mrj6OUClkOJWWZ5BU2CnleLmPH17u7Qd7GzkmrTqE3SlZ6ORpj5+n9TaYeFCgKUX0tjOIP3NTnyxX5aRSoJOXPR52t4OPkwqtnGzQykkFb0cbKBV3ziUIwOKd5/D13vJhxYEd3BA9KgATYhJxNiMfwb7OWDclBHKZFGU6ARHrkvHbyQzYKeV4yL2Fvp7wEW97fDgiAB087vybm1ukxaRVh3AmXQ17azlWTgpGYBunOmNb6XxmPgZ/vhcKmQQLemjx3NPGvzcyQTKx5poghX2+p1p3NwB09LTHz9MetYihtaoqiz6fDPBE1PDOtR7XmFhfyS7E4p3n0Kd9ywYnRnf77s9L+HDrGdhaybBpWu8ak7iF28/iq90XIZEAS0Z3b/DaUdkFGvT7dDcKNKX4z+huKNMJNfYeGcvVvwvxyW9n8dvJ8ocKD+zghu/CgyCRSCzuz/X9KtCUoueHO3FbW4aNbzyKq38XYsb6Y2ihlGPP//WDSwvjTGm/28kbeRi5bD8EQcBrj7fDtP4PVevFKdMJ2H02A38mHELE84Pgat/4WWma0jJcyirU/2KkLi5FRt5trDmQqr8he9hbo7WLComXcwAAz3TzwkfPBtT5Zys59RaKS8rw6EP3Ho4yl3/8eAz/S76OYD9nLBvXo97f391/rtUaHUZ89Reu5dyu8Vh3eyVG9miFUT1a6X8JEQQBaXnlEztuFZVgSCcPg57RmxU9XLeKtHijXzu8M7T8F9LTaWpMW5eMy9l3hp19XVTo5GWPjh726Oxtj06eDnC3Vzao6P7nIzfw7v+OQ1Oqg7VCimKtDq4trLD1rcf0tZRAecI+ISYRiVfKv3eVlQz/CPNHeGibGv9NzLutxcurDiHpavkaXN9MCKz3MPeiuHNYEn8e/f1dMcI5wyT/htT3/s0ibdLLu63VJ0fT+reDk8oK9tYKOKgUePzhlhaZHAFAJy97HJw9yCTn9nW1xeLR3e/7POGP+kJTqkOvts619nD9I8wfng7W8HW1bVTNjGsLJV57vC3+veMcFm5P0f/2ObVvO6MnRwDQxsUWy14KxKErOUi4+DfCQ31FmQ5sDi2UcjzxiAc2HrmBtQdSceDS3wCA1/u1M0lyBJQ/vHnnjMehkEvg6VBzb5BMKkHf9q4oOC/cV6E1ACjlshqHQ6b0bYsfD13D0j8uIkNdjAx1MZQVQ2r1ffRPj9b170Ewh0+f64KJj/qio6ddo3/pcWmhxIrwnvjHhmOQSSVoVdFj4+1og3YtWyDYz7laj6FEIoG3o02tvXtu9taIHhmA19YkY/mei+jfwQ3nMvMx/5fTKCnVwcvBGh8++wiC/VyM8nd6RHdvtGvZAlO/P4z0vGJIK345uzs5AsoLur+dEITIH4+ihbUc7w7tYFCLV5WDjQLfTw7Gq98n4c/z2Zi86jBWTuqJ3nUkyIIg4NfjaQCAYY94AGkZ932N94MJEumduJ4HoHxNGEsbSmvqFDJpnev8yKQSjA/1va/PmdynLdYcSMWN3PLfap1UCkwIvf+ZNvfS09e51rVsmpNRga2w8cgN/eNjPB2sa63xMZaaCqzNTSmXYXyoL17o6YMfD1/Hocs5eL1fu0bVllgKqVSCgFb1H8qrTXt3O2yJ6GOEFt0x9BFPPBfYCj8lXcdL3x2EpuKRRwM6uOGz57vqZ7oaS0ArB2yJ6IPFO88h2M+51l4+B5UCMRN71rivJiorOb4LD8Kb645gx+lMLN9zsc4E6XS6GpeyCmEll5avk5bWoEsxOj5qhPSOXc8FgHtO4SfLZmMlwz/CHta/ntq3XbXp+NQ4oW1d4HXXKsz/DPM3SgF4U6GUyzC+VxssGdO9SSdHTUHU8E7wdrSBplQHuVSC2U92wHcTgoyeHFVqaafER88G4Jlu1df5uh9KuUw/THjwUk6djxX6tWLGXH//lrCzFv/fLSZIpFe5pk5XI/xmReIZ2aMVBnRwQ5dWDibvPXqQSKUSjAosX0yzk6c9nq1h0UgiY7CzVuC78CC8ENQK618NxdS+7Yy+NpK5tGtpi9bOKpSU6fDXhexaj7t7eK2xz+40NvFTNLIYxyuG2CxpGj81nEwqwYoGdIVT/b3erx2sFTI83dWryd6wqGno6GmPhc91FbsZ900ikaC/f0usTriKP1JuIqyzR43HHbueh2s5t2GjkGFABzcA4s8fYw8SAShfHyRDXV6k94g3u8+JaqKykmNa/4eM/gwzouascrmSP85mobaJ878eK+89GtjRrdGP2DE2JkgE4M7w2sPudhbzh5OIiJq+Xm1dYK2QIkNdvgBnVTqdgK0nyuuPqq7sLSYmSATg7uE11h8REZHxWCtk6N2ufAbbHyk3q+0/dCUH6XnFaKGUo59/4x8LZGxMkAjAnRlsrD8iIiJj66cfZqueIC3dfRFA+YPGLWlmKBMkgiAI+h6krkyQiIjIyPpX9Awlp95CbtGdh+cmXs7B3nNZkEslda4VZ25MkAhX/y5C3m0trORS+HvU/oBEIiKixmjlpMLD7i2gE6B/vqUgCPj3jhQAwAs9fSxiYdS7MUEi/fBaJ097g4cjEhERGUvlbLbdKeUJ0r4L2Ui8nAMruRRvDrCs3iOACRIBdw2vsUCbiIhMo79/ZYJ0E2U6Af/ecQ4A8FJIm1qfNygmJkikn+LPAm0iIjKVwDZOsLOW41aRFp/tSMGxa7mwUcjwer92YjetRkyQHnClZTqcTKvoQeIz2IiIyEQUMin6PlxerP1Vxcy1ib190dJOKWazasUE6QF3/mYBirU62CnlaOtqK3ZziIioGascZgMAO6Ucr/ZtK2Jr7o0J0gOucnjtEW8HPluKiIhM6u6FIF95rC0cVVYitubeRE+Qli5dCl9fX1hbWyMkJASJiYn3PH7x4sXw9/eHjY0NfHx8MGPGDBQXF+v3R0dHo2fPnrCzs4ObmxtGjBiBlJQUg3P069cPEonE4Oe1114zyfVZumOVK2j7sECbiIhMy7WFEpN6++LRdi54uY+v2M25J1EfurV+/XpERkZi+fLlCAkJweLFizFkyBCkpKTAzc2t2vHr1q3DzJkzsWLFCjz66KM4d+4cJk6cCIlEgkWLFgEA9uzZg2nTpqFnz54oLS3F7NmzERYWhtOnT8PW9s4Q0pQpU7BgwQL9a5XKstZfMJfjFVP8u7FAm4iIzCBqeGexm1AvoiZIixYtwpQpUzBp0iQAwPLly7F161asWLECM2fOrHb8/v370bt3b4wdOxYA4OvrizFjxuDgwYP6Y7Zv327wnlWrVsHNzQ1JSUno27evfrtKpYKHh4cpLqvJKNaWISWj/MGBXVigTUREpCdaglRSUoKkpCTMmjVLv00qlWLQoEFISEio8T2PPvoo1qxZg8TERAQHB+PSpUvYtm0bxo8fX+vn5OWVDyE5OzsbbF+7di3WrFkDDw8PDB8+HHPnzr1nL5JGo4FGo9G/VqvVAACtVgutVlv3Bdei8r33c47GOp6ai1KdABdbK7RUyURpgzmJGesHDWNtPoy1+TDW5mPKWNf3nKIlSNnZ2SgrK4O7u7vBdnd3d5w9e7bG94wdOxbZ2dno06cPBEFAaWkpXnvtNcyePbvG43U6Hd5++2307t0bjzzyiMF52rRpAy8vLxw/fhzvvvsuUlJSsHHjxlrbGx0djfnz51fbvmPHDqMMz8XFxd33ORpqy1UpACk8rYrx22+/mf3zxSJGrB9UjLX5MNbmw1ibjyliXVRUVK/jRB1ia6jdu3fj448/xldffYWQkBBcuHAB06dPxwcffIC5c+dWO37atGk4efIk9u3bZ7B96tSp+v8PCAiAp6cnBg4ciIsXL6Jdu5oXrJo1axYiIyP1r9VqNXx8fBAWFgZ7e/tGX5NWq0VcXBwGDx4MhULR6PM0lPq2FrM/2wugDBFP9sDADtVrvpobsWL9IGKszYexNh/G2nxMGevKEaC6iJYgubq6QiaTITMz02B7ZmZmrbVBc+fOxfjx4/HKK68AKE9uCgsLMXXqVLz33nuQSu9MyouIiMCvv/6KvXv3olWrVvdsS0hICADgwoULtSZISqUSSmX1xawUCoVRvjxjnae+YvddRaGmDP7udgjr7PVATfE3d6wfZIy1+TDW5sNYm48pYl3f84k2zd/KygqBgYGIj4/Xb9PpdIiPj0doaGiN7ykqKjJIggBAJpMBKH8qcOV/IyIisGnTJuzatQt+fn51tuXo0aMAAE9Pz8ZcSpNzu6QMK/ZdBgC81q/tA5UcERER1YeoQ2yRkZEIDw9HUFAQgoODsXjxYhQWFupntU2YMAHe3t6Ijo4GAAwfPhyLFi1C9+7d9UNsc+fOxfDhw/WJ0rRp07Bu3Tps3rwZdnZ2yMjIAAA4ODjAxsYGFy9exLp16/Dkk0/CxcUFx48fx4wZM9C3b1906dJFnECY2Y+Hr+HvwhK0crLB8C5eYjeHiIjI4oiaIL344ovIysrCvHnzkJGRgW7dumH79u36wu3U1FSDHqM5c+ZAIpFgzpw5uHHjBlq2bInhw4fjo48+0h+zbNkyAOWLQd5t5cqVmDhxIqysrLBz5059Mubj44NRo0Zhzpw5pr9gC6At0+GbvZcAAK/2bQu5TPS1QomIiCyO6EXaERERiIiIqHHf7t27DV7L5XJERUUhKiqq1vNVDrXVxsfHB3v27GlwO5uLX46l4Ububbi2sMLzQT5iN4eIiMgisfvgAaLTCVhW8QTlSb39YK2QidwiIiIiy8QE6QESf/Ymzt8sQAulHC/1aiN2c4iIiCwWE6QHyNd7ynuPXurVBg42nKJKRERUGyZIDwh1sRaHr94CAEx81FfcxhAREVk4JkgPiFM3ylcO9Xa0gYeDtcitISIismxMkB4QJ2+UP7T3Ee/GPxaFiIjoQcEE6QFxvCJB6tLKUdyGEBERNQFMkB4Qd3qQHERuCRERkeVjgvQAUBdrcTm7EAAQwASJiIioTkyQHgB3F2g721qJ3BoiIiLLxwTpAXDiRi4A9h4RERHVFxOkB8CJih6kgFZMkIiIiOqDCdIDgAXaREREDcMEqZljgTYREVHDMUFq5ip7j1igTUREVH9MkJq5ygSJvUdERET1xwSpmWOBNhERUcMxQWrm2INERETUcEyQmjEWaBMRETUOE6Rm7O4CbScWaBMREdUbE6RmjMNrREREjcMEqRljgTYREVHjMEFqxk5czwXAHiQiIqKGYoLUTKmLtbjydxEAJkhEREQNxQSpmWKBNhERUeMxQWqmKhOkLqw/IiIiajAmSM3U0Wu5AFigTURE1BhMkJqpI6m5AIDuPk7iNoSIiKgJYoLUDGWqi5GeVwyphENsREREjcEEqRmq7D162N0Otkq5uI0hIiJqgpggNUNHrt0CAHRvzeE1IiKixmCC1Awd1dcfOYraDiIioqaKCVIzU1qmw/Hr5VP8u7V2FLcxRERETRQTpGbmXGYBbmvLYKeU46GWLcRuDhERUZPEBKmZqaw/6uLjAKlUInJriIiImiYmSM3MUa5/REREdN+YIDUzRypW0O7GAm0iIqJGEz1BWrp0KXx9fWFtbY2QkBAkJibe8/jFixfD398fNjY28PHxwYwZM1BcXNygcxYXF2PatGlwcXFBixYtMGrUKGRmZhr92sxNXazFxawCACzQJiIiuh+iJkjr169HZGQkoqKikJycjK5du2LIkCG4efNmjcevW7cOM2fORFRUFM6cOYOYmBisX78es2fPbtA5Z8yYgV9++QUbNmzAnj17kJaWhpEjR5r8ek3t+LU8CALg42wD1xZKsZtDRETUZImaIC1atAhTpkzBpEmT0KlTJyxfvhwqlQorVqyo8fj9+/ejd+/eGDt2LHx9fREWFoYxY8YY9BDVdc68vDzExMRg0aJFGDBgAAIDA7Fy5Urs378fBw4cMMt1m8qR1IoFIll/REREdF9Eew5FSUkJkpKSMGvWLP02qVSKQYMGISEhocb3PProo1izZg0SExMRHByMS5cuYdu2bRg/fny9z5mUlAStVotBgwbpj+nQoQNat26NhIQE9OrVq8bP1mg00Gg0+tdqtRoAoNVqodVqGxkF6N97P+eolJyaAwAI8LYzyvmaG2PGmu6NsTYfxtp8GGvzMWWs63tO0RKk7OxslJWVwd3d3WC7u7s7zp49W+N7xo4di+zsbPTp0weCIKC0tBSvvfaafoitPufMyMiAlZUVHB0dqx2TkZFRa3ujo6Mxf/78att37NgBlUpV5/XWJS4u7r7eLwhA4kUZAAmKUk9h261T992m5up+Y031x1ibD2NtPoy1+Zgi1kVFRfU6rkk9yXT37t34+OOP8dVXXyEkJAQXLlzA9OnT8cEHH2Du3Lkm/exZs2YhMjJS/1qtVsPHxwdhYWGwt7dv9Hm1Wi3i4uIwePBgKBSKRp8nNacIhQf2QSGTYPKooVDKRa+/tzjGijXVjbE2H8bafBhr8zFlrCtHgOoiWoLk6uoKmUxWbfZYZmYmPDw8anzP3LlzMX78eLzyyisAgICAABQWFmLq1Kl477336nVODw8PlJSUIDc316AX6V6fCwBKpRJKZfXCZ4VCYZQv737PczK9fPZaJy8HtLBhgfa9GOs7o7ox1ubDWJsPY20+poh1fc8nWjeDlZUVAgMDER8fr9+m0+kQHx+P0NDQGt9TVFQEqdSwyTKZDAAgCEK9zhkYGAiFQmFwTEpKClJTU2v93KbgCB9QS0REZDSiDrFFRkYiPDwcQUFBCA4OxuLFi1FYWIhJkyYBACZMmABvb29ER0cDAIYPH45Fixahe/fu+iG2uXPnYvjw4fpEqa5zOjg4YPLkyYiMjISzszPs7e3x5ptvIjQ0tNYC7aagcoHI7lz/iIiI6L6JmiC9+OKLyMrKwrx585CRkYFu3bph+/bt+iLr1NRUgx6jOXPmQCKRYM6cObhx4wZatmyJ4cOH46OPPqr3OQHg888/h1QqxahRo6DRaDBkyBB89dVX5rtwIysp1eFMWvmYKqf4ExER3T/Ri7QjIiIQERFR477du3cbvJbL5YiKikJUVFSjzwkA1tbWWLp0KZYuXdrg9lqim/nFKCnTQSGTwMfZRuzmEBERNXmc6tQMZBeUAABcWyghkUhEbg0REVHTxwSpGcjOL1/Ako8XISIiMg4mSM1AdkFlgmQlckuIiIiaByZIzUBlgtTSjj1IRERExsAEqRm4uwaJiIiI7h8TpGYgizVIRERERsUEqRnIqqxB4hAbERGRUTBBagZYpE1ERGRcTJCagcpp/m7sQSIiIjIKJkhNnKa0DOriUgCsQSIiIjIWJkhN3N8VM9gUMgkcbBQit4aIiKh5YILUxFXOYHOx5WNGiIiIjIUJUhOnL9C2Y4E2ERGRsTBBauL0q2iz/oiIiMhomCA1cVxFm4iIyPiYIDVx+lW0OcWfiIjIaJggNXF3FolkgkRERGQsTJCauDvPYWORNhERkbEwQWriWKRNRERkfEyQmrjKIu2WrEEiIiIyGiZITVhJqQ55t7UAWINERERkTEyQmrC/C8uH1+RSPmaEiIjImJggNWHZ+eXDay4trCCV8jEjRERExsIEqQnLKigGwOE1IiIiY2OC1IRV9iCxQJuIiMi4mCA1YVlcJJKIiMgkmCA1YVxFm4iIyDSYIDVhdx5Uy1W0iYiIjIkJUhOWXfGYEdYgERERGRcTpCYsi48ZISIiMgkmSE2YvgaJPUhERERGxQSpidKW6ZBbxMeMEBERmQITpCbq74oCbZlUAkc+ZoSIiMiomCA1UZXDay62fMwIERGRsTFBaqKy8rkGEhERkakwQWqi9DPYWKBNRERkdEyQmiiuok1ERGQ6FpEgLV26FL6+vrC2tkZISAgSExNrPbZfv36QSCTVfoYNG6Y/pqb9EokEn376qf4YX1/favs/+eQTk16nMVU+qNbVjqtoExERGZtc7AasX78ekZGRWL58OUJCQrB48WIMGTIEKSkpcHNzq3b8xo0bUVJSon/9999/o2vXrnj++ef129LT0w3e89tvv2Hy5MkYNWqUwfYFCxZgypQp+td2dnbGuiyTy+YikURERCYjeoK0aNEiTJkyBZMmTQIALF++HFu3bsWKFSswc+bMasc7OzsbvI6NjYVKpTJIkDw8PAyO2bx5M/r374+2bdsabLezs6t2bG00Gg00Go3+tVqtBgBotVpotdp6naMmle9t6Dmy8osBAI428vv6/AdJY2NNDcdYmw9jbT6MtfmYMtb1PadEEATB6J9eTyUlJVCpVPjpp58wYsQI/fbw8HDk5uZi8+bNdZ4jICAAoaGh+Oabb2rcn5mZiVatWmH16tUYO3asfruvry+Ki4uh1WrRunVrjB07FjNmzIBcXnPO+P7772P+/PnVtq9btw4qlarOdhpb9FEZMm5L8EanMvg7iPYVEhERNSlFRUUYO3Ys8vLyYG9vX+txovYgZWdno6ysDO7u7gbb3d3dcfbs2Trfn5iYiJMnTyImJqbWY1avXg07OzuMHDnSYPtbb72FHj16wNnZGfv378esWbOQnp6ORYsW1XieWbNmITIyUv9arVbDx8cHYWFh9wxwXbRaLeLi4jB48GAoFPVf8PH9Y38A0GLYgD542L3pDA2KqbGxpoZjrM2HsTYfxtp8TBnryhGguog+xHY/YmJiEBAQgODg4FqPWbFiBcaNGwdra2uD7XcnO126dIGVlRVeffVVREdHQ6msXtejVCpr3K5QKIzy5TXkPNoyHW5VPGbEw9GWf1EbyFjfGdWNsTYfxtp8GGvzMUWs63s+UWexubq6QiaTITMz02B7ZmZmnbVBhYWFiI2NxeTJk2s95s8//0RKSgpeeeWVOtsSEhKC0tJSXLlypV5tF1NO4Z3HjDipOIuNiIjI2ERNkKysrBAYGIj4+Hj9Np1Oh/j4eISGht7zvRs2bIBGo8FLL71U6zExMTEIDAxE165d62zL0aNHIZVKa5w5Z2kqV9F25mNGiIiITEL0IbbIyEiEh4cjKCgIwcHBWLx4MQoLC/Wz2iZMmABvb29ER0cbvC8mJgYjRoyAi4tLjedVq9XYsGEDPvvss2r7EhIScPDgQfTv3x92dnZISEjAjBkz8NJLL8HJycn4F2lknOJPRERkWg1OkHx9ffHyyy9j4sSJaN269X034MUXX0RWVhbmzZuHjIwMdOvWDdu3b9cXbqempkIqNezoSklJwb59+7Bjx45azxsbGwtBEDBmzJhq+5RKJWJjY/H+++9Do9HAz88PM2bMMKhLsmT657DxMSNEREQm0eAE6e2338aqVauwYMEC9O/fH5MnT8azzz5bYwFzfUVERCAiIqLGfbt37662zd/fH3WtTjB16lRMnTq1xn09evTAgQMHGtxOS5FdULGKdgvWHxEREZlCg2uQ3n77bRw9ehSJiYno2LEj3nzzTXh6eiIiIgLJycmmaCNVwSE2IiIi02p0kXaPHj2wZMkSpKWlISoqCt999x169uyJbt26YcWKFXX28FDjVc5ic2EPEhERkUk0ukhbq9Vi06ZNWLlyJeLi4tCrVy9MnjwZ169fx+zZs7Fz506sW7fOmG2lCpUJEqf4ExERmUaDE6Tk5GSsXLkSP/zwA6RSKSZMmIDPP/8cHTp00B/z7LPPomfPnkZtKN1xq6g8QXK2ZYJERERkCg1OkHr27InBgwdj2bJlGDFiRI0rUvr5+WH06NFGaSBVV5kgObIHiYiIyCQanCBdunQJbdq0uecxtra2WLlyZaMbRfd2q7D8MSPsQSIiIjKNBhdp37x5EwcPHqy2/eDBgzh8+LBRGkW1KynVoUBTCgBwUvFZQERERKbQ4ARp2rRpuHbtWrXtN27cwLRp04zSKKpdbsXwmlQC2FszQSIiIjKFBidIp0+fRo8ePapt7969O06fPm2URlHtbhWVD685qvgcNiIiIlNpcIKkVCqRmZlZbXt6ejrkctEf7dbs3Zniz94jIiIiU2lwghQWFoZZs2YhLy9Pvy03NxezZ8/G4MGDjdo4qq5yiI1rIBEREZlOg7t8/v3vf6Nv375o06YNunfvDgA4evQo3N3d8f333xu9gWQopzJB4gw2IiIik2lwguTt7Y3jx49j7dq1OHbsGGxsbDBp0iSMGTOmxjWRyLhyK2qQOMRGRERkOo0qGrK1tcXUqVON3RaqB30NEnuQiIiITKbRVdWnT59GamoqSkpKDLY//fTT990oqt0t1iARERGZXKNW0n722Wdx4sQJSCQSCIIAAJBIyqecl5WVGbeFZOBWRQ+SMxMkIiIik2nwLLbp06fDz88PN2/ehEqlwqlTp7B3714EBQVh9+7dJmgi3S1Hvw4Sa5CIiIhMpcE9SAkJCdi1axdcXV0hlUohlUrRp08fREdH46233sKRI0dM0U6qUDnNn89hIyIiMp0G9yCVlZXBzs4OAODq6oq0tDQAQJs2bZCSkmLc1lE1LNImIiIyvQb3ID3yyCM4duwY/Pz8EBISgoULF8LKygrffPMN2rZta4o2UgVtmQ75xZUPqmWCREREZCoNTpDmzJmDwsJCAMCCBQvw1FNP4bHHHoOLiwvWr19v9AbSHZVrIEkkgIMNa5CIiIhMpcEJ0pAhQ/T//9BDD+Hs2bPIycmBk5OTfiYbmUZl/ZGDjQIyPqiWiIjIZBpUg6TVaiGXy3Hy5EmD7c7OzkyOzCCHU/yJiIjMokEJkkKhQOvWrbnWkUhucYo/ERGRWTR4Ftt7772H2bNnIycnxxTtoXu4xSn+REREZtHgGqQvv/wSFy5cgJeXF9q0aQNbW1uD/cnJyUZrHBmqTJAcOcRGRERkUg1OkEaMGGGCZlB96B8zwh4kIiIik2pwghQVFWWKdlA9sAaJiIjIPBpcg0Ti4YNqiYiIzKPBPUhSqfSeU/o5w810cliDREREZBYNTpA2bdpk8Fqr1eLIkSNYvXo15s+fb7SGUXWVK2mzBomIiMi0GpwgPfPMM9W2Pffcc+jcuTPWr1+PyZMnG6VhVJ3+QbWsQSIiIjIpo9Ug9erVC/Hx8cY6HVVRWqaDuri8B8mJPUhEREQmZZQE6fbt21iyZAm8vb2NcTqqQd5tLQSh/P8d+aBaIiIik2rwEFvVh9IKgoD8/HyoVCqsWbPGqI2jOyqn+NtbyyGXcfIhERGRKTU4Qfr8888NEiSpVIqWLVsiJCQETk5ORm0c3VG5ijaH14iIiEyvwV0REydORHh4uP5n/PjxGDp06H0lR0uXLoWvry+sra0REhKCxMTEWo/t168fJBJJtZ9hw4YZtLHq/qFDhxqcJycnB+PGjYO9vT0cHR0xefJkFBQUNPoaTO2WvkCbCRIREZGpNThBWrlyJTZs2FBt+4YNG7B69eoGN2D9+vWIjIxEVFQUkpOT0bVrVwwZMgQ3b96s8fiNGzciPT1d/3Py5EnIZDI8//zzBscNHTrU4LgffvjBYP+4ceNw6tQpxMXF4ddff8XevXsxderUBrffXPQ9SJzBRkREZHINTpCio6Ph6upabbubmxs+/vjjBjdg0aJFmDJlCiZNmoROnTph+fLlUKlUWLFiRY3HOzs7w8PDQ/8TFxcHlUpVLUFSKpUGx93dw3XmzBls374d3333HUJCQtCnTx988cUXiI2NRVpaWoOvwRwqa5A4xEZERGR6Da5BSk1NhZ+fX7Xtbdq0QWpqaoPOVVJSgqSkJMyaNUu/TSqVYtCgQUhISKjXOWJiYjB69GjY2toabN+9ezfc3Nzg5OSEAQMG4MMPP4SLiwsAICEhAY6OjggKCtIfP2jQIEilUhw8eBDPPvtstc/RaDTQaDT612q1GkD5Qplarbb+F11F5XvrOkd2fjEAwNFafl+f9yCrb6zp/jHW5sNYmw9jbT6mjHV9z9ngBMnNzQ3Hjx+Hr6+vwfZjx47pE5D6ys7ORllZGdzd3Q22u7u74+zZs3W+PzExESdPnkRMTIzB9qFDh2LkyJHw8/PDxYsXMXv2bDzxxBNISEiATCZDRkYG3NzcDN4jl8vh7OyMjIyMGj8rOjq6xpXCd+zYAZVKVWdb6xIXF3fP/ScuSAFIkXntErZtu3jfn/cgqyvWZDyMtfkw1ubDWJuPKWJdVFRUr+ManCCNGTMGb731Fuzs7NC3b18AwJ49ezB9+nSMHj26oae7LzExMQgICEBwcLDB9rvbERAQgC5duqBdu3bYvXs3Bg4c2KjPmjVrFiIjI/Wv1Wo1fHx8EBYWBnt7+8ZdAMoz2bi4OAwePBgKRe31RVvWHgGystCrewCe7Nmq0Z/3IKtvrOn+Mdbmw1ibD2NtPqaMdeUIUF0anCB98MEHuHLlCgYOHAi5vPztOp0OEyZMaHANkqurK2QyGTIzMw22Z2ZmwsPD457vLSwsRGxsLBYsWFDn57Rt2xaurq64cOECBg4cCA8Pj2pF4KWlpcjJyan1c5VKJZRKZbXtCoXCKF9eXefJvV0KAHC1s+ZfzPtkrO+M6sZYmw9jbT6MtfmYItb1PV+Di7StrKywfv16pKSkYO3atdi4cSMuXryIFStWwMqqYQXEVlZWCAwMNHhEiU6nQ3x8PEJDQ+/53g0bNkCj0eCll16q83OuX7+Ov//+G56engCA0NBQ5ObmIikpSX/Mrl27oNPpEBIS0qBrMBeug0RERGQ+De5BqtS+fXu0b9/+vhsQGRmJ8PBwBAUFITg4GIsXL0ZhYSEmTZoEAJgwYQK8vb0RHR1t8L6YmBiMGDGiWt1TQUEB5s+fj1GjRsHDwwMXL17EO++8g4ceeghDhgwBAHTs2BFDhw7FlClTsHz5cmi1WkRERGD06NHw8vK672syBa6DREREZD4NTpBGjRqF4OBgvPvuuwbbFy5ciEOHDtW4RtK9vPjii8jKysK8efOQkZGBbt26Yfv27frC7dTUVEilhh1dKSkp2LdvH3bs2FHtfDKZDMePH8fq1auRm5sLLy8vhIWF4YMPPjAYIlu7di0iIiIwcOBASKVSjBo1CkuWLGlQ282lTCcg73blNH926xIREZlagxOkvXv34v3336+2/YknnsBnn33WqEZEREQgIiKixn27d++uts3f3x9C5ZNbq7CxscHvv/9e52c6Oztj3bp1DWqnWNS3tdDpH1TLHiQiIiJTa3ANUkFBQY21RgqFot6V4dQwlfVHdko5rOR8UC0REZGpNfhuGxAQgPXr11fbHhsbi06dOhmlUWSoMkFy5PAaERGRWTR4iG3u3LkYOXIkLl68iAEDBgAA4uPjsW7dOvz0009GbyABtwrL64+cWaBNRERkFg1OkIYPH46ff/4ZH3/8MX766SfY2Niga9eu2LVrF5ydnU3RxgdeTmUPEhMkIiIis2jUNP9hw4Zh2LBhAMpXpPzhhx/wz3/+E0lJSSgrKzNqAwnIrUiQnLkGEhERkVk0uuJ37969CA8Ph5eXFz777DMMGDAABw4cMGbbqEJOxRCbo4o1SERERObQoB6kjIwMrFq1CjExMVCr1XjhhReg0Wjw888/s0DbhCoXiWQNEhERkXnUuwdp+PDh8Pf3x/Hjx7F48WKkpaXhiy++MGXbqMKdWWxMkIiIiMyh3j1Iv/32G9566y28/vrrRnnECNVfZYLEHiQiIiLzqHcP0r59+5Cfn4/AwECEhITgyy+/RHZ2tinbRhVuFVU8ZoQ1SERERGZR7wSpV69e+Pbbb5Geno5XX30VsbGx8PLygk6nQ1xcHPLz803Zzgea/kG1HGIjIiIyiwbPYrO1tcXLL7+Mffv24cSJE/jHP/6BTz75BG5ubnj66adN0cYHmk4nILfiQbWc5k9ERGQe9/VgL39/fyxcuBDXr1/HDz/8YKw20V3yi0tRVvGkWk7zJyIiMg+jPPlUJpNhxIgR2LJlizFOR3epLNBWWcmglMtEbg0REdGDgY+Gt3AFmlIAQAtloxY9JyIiokZggmThNKXlj26xVrD3iIiIyFyYIFk4jVYHALBW8KsiIiIyF951LVxxRQ8S64+IiIjMhwmShStmDxIREZHZ8a5r4Yq1rEEiIiIyNyZIFk5TWt6DxCE2IiIi82GCZOEqe5CUHGIjIiIyG951LZy+Bok9SERERGbDBMnC3alB4ldFRERkLrzrWrjKGiQWaRMREZkPEyQLp69BkvOrIiIiMhfedS0cHzVCRERkfkyQLBwXiiQiIjI/3nUtHHuQiIiIzI8JkoWr7EFiDRIREZH58K5r4fioESIiIvNjgmTh7sxiY4JERERkLkyQLNyddZD4VREREZkL77oWjkNsRERE5scEycKxSJuIiMj8eNe1cJzmT0REZH5MkCzcnYUimSARERGZi0UkSEuXLoWvry+sra0REhKCxMTEWo/t168fJBJJtZ9hw4YBALRaLd59910EBATA1tYWXl5emDBhAtLS0gzO4+vrW+0cn3zyiUmvszHu9CBZxFdFRET0QBD9rrt+/XpERkYiKioKycnJ6Nq1K4YMGYKbN2/WePzGjRuRnp6u/zl58iRkMhmef/55AEBRURGSk5Mxd+5cJCcnY+PGjUhJScHTTz9d7VwLFiwwONebb75p0mttqDKdAG2ZAIDT/ImIiMxJLnYDFi1ahClTpmDSpEkAgOXLl2Pr1q1YsWIFZs6cWe14Z2dng9exsbFQqVT6BMnBwQFxcXEGx3z55ZcIDg5GamoqWrdurd9uZ2cHDw+PerVTo9FAo9HoX6vVagDlPVZarbZe56hJ5XtrOkehplT//zKU3dfn0L1jTcbFWJsPY20+jLX5mDLW9T2nRBAEweifXk8lJSVQqVT46aefMGLECP328PBw5ObmYvPmzXWeIyAgAKGhofjmm29qPWbnzp0ICwtDbm4u7O3tAZQPsRUXF0Or1aJ169YYO3YsZsyYAbm85pzx/fffx/z586ttX7duHVQqVZ3tbIwCLfDe4fL2fN6rFFKJST6GiIjogVFUVISxY8ciLy9PnxPURNQepOzsbJSVlcHd3d1gu7u7O86ePVvn+xMTE3Hy5EnExMTUekxxcTHeffddjBkzxiAQb731Fnr06AFnZ2fs378fs2bNQnp6OhYtWlTjeWbNmoXIyEj9a7VaDR8fH4SFhd0zwHXRarWIi4vD4MGDoVAoDPal5xUDh/dCIZPgqWFPNvozqNy9Yk3GxVibD2NtPoy1+Zgy1pUjQHURfYjtfsTExCAgIADBwcE17tdqtXjhhRcgCAKWLVtmsO/uZKdLly6wsrLCq6++iujoaCiVymrnUiqVNW5XKBRG+fJqOk+pUD6kZy2X8S+jERnrO6O6Mdbmw1ibD2NtPqaIdX3PJ2qRtqurK2QyGTIzMw22Z2Zm1lkbVFhYiNjYWEyePLnG/ZXJ0dWrVxEXF1dnL09ISAhKS0tx5cqVBl2DKekXieQUfyIiIrMSNUGysrJCYGAg4uPj9dt0Oh3i4+MRGhp6z/du2LABGo0GL730UrV9lcnR+fPnsXPnTri4uNTZlqNHj0IqlcLNza3hF2IixZziT0REJArRh9giIyMRHh6OoKAgBAcHY/HixSgsLNTPapswYQK8vb0RHR1t8L6YmBiMGDGiWvKj1Wrx3HPPITk5Gb/++ivKysqQkZEBoHwGnJWVFRISEnDw4EH0798fdnZ2SEhIwIwZM/DSSy/BycnJPBdeDxouEklERCQK0ROkF198EVlZWZg3bx4yMjLQrVs3bN++XV+4nZqaCqnUsAclJSUF+/btw44dO6qd78aNG9iyZQsAoFu3bgb7/vjjD/Tr1w9KpRKxsbF4//33odFo4OfnhxkzZhjUJVmCyh4kPoeNiIjIvERPkAAgIiICERERNe7bvXt3tW3+/v6obXUCX1/fWvdV6tGjBw4cONDgdpqbRsvnsBEREYmBXRMW7M5z2Pg1ERERmRPvvBZM/xw2PmaEiIjIrJggWbBiFmkTERGJggmSBSvWskibiIhIDLzzWjAuFElERCQOJkgWTMOFIomIiETBO68FYw0SERGROJggWTAuFElERCQO3nktWDEXiiQiIhIFEyQLpn8WG3uQiIiIzIp3Xgt2p0ibPUhERETmxATJgt2Z5s+viYiIyJx457Vg+hokPmqEiIjIrJggWbBiDrERERGJggmSBdNwiI2IiEgUvPNasDvrILEHiYiIyJyYIFmwOytp82siIiIyJ955LRgXiiQiIhIHEyQLpinls9iIiIjEwATJQul0AkpKuZI2ERGRGHjntVCVvUcAoGQPEhERkVkxQbJQlfVHAHuQiIiIzI13XgtV2YMkl0ogl/FrIiIiMifeeS0UZ7ARERGJhwmShbqzSCS/IiIiInPj3ddC3Vkkkj1IRERE5sYEyUJVDrHxOWxERETmx7uvhdIvEsnnsBEREZkdEyQLxR4kIiIi8fDua6H0s9jYg0RERGR2TJAslEZfpM2viIiIyNx497VQmlKug0RERCQWJkgWqnKaP9dBIiIiMj/efS0UV9ImIiISDxMkC1XMITYiIiLRMEGyUJVF2pzmT0REZH4WcfddunQpfH19YW1tjZCQECQmJtZ6bL9+/SCRSKr9DBs2TH+MIAiYN28ePD09YWNjg0GDBuH8+fMG58nJycG4ceNgb28PR0dHTJ48GQUFBSa7xobS9yBxmj8REZHZiZ4grV+/HpGRkYiKikJycjK6du2KIUOG4ObNmzUev3HjRqSnp+t/Tp48CZlMhueff15/zMKFC7FkyRIsX74cBw8ehK2tLYYMGYLi4mL9MePGjcOpU6cQFxeHX3/9FXv37sXUqVNNfr31VcweJCIiItGIfvddtGgRpkyZgkmTJqFTp05Yvnw5VCoVVqxYUePxzs7O8PDw0P/ExcVBpVLpEyRBELB48WLMmTMHzzzzDLp06YL//ve/SEtLw88//wwAOHPmDLZv347vvvsOISEh6NOnD7744gvExsYiLS3NXJd+T1wokoiISDxyMT+8pKQESUlJmDVrln6bVCrFoEGDkJCQUK9zxMTEYPTo0bC1tQUAXL58GRkZGRg0aJD+GAcHB4SEhCAhIQGjR49GQkICHB0dERQUpD9m0KBBkEqlOHjwIJ599tlqn6PRaKDRaPSv1Wo1AECr1UKr1Tbswu9S+d6q5yguKQUAKKTV91Hj1BZrMj7G2nwYa/NhrM3HlLGu7zlFTZCys7NRVlYGd3d3g+3u7u44e/Zsne9PTEzEyZMnERMTo9+WkZGhP0fVc1buy8jIgJubm8F+uVwOZ2dn/TFVRUdHY/78+dW279ixAyqVqs621iUuLs7g9fV0KQApUk6fwLas4/d9frqjaqzJdBhr82GszYexNh9TxLqoqKhex4maIN2vmJgYBAQEIDg42OSfNWvWLERGRupfq9Vq+Pj4ICwsDPb29o0+r1arRVxcHAYPHgyFQqHf/n1aIpCXi+DA7njiEY/7ajuVqy3WZHyMtfkw1ubDWJuPKWNdOQJUF1ETJFdXV8hkMmRmZhpsz8zMhIfHvZOCwsJCxMbGYsGCBQbbK9+XmZkJT09Pg3N269ZNf0zVIvDS0lLk5OTU+rlKpRJKpbLadoVCYZQvr+p5SsoEAICttRX/IhqZsb4zqhtjbT6Mtfkw1uZjiljX93yiFmlbWVkhMDAQ8fHx+m06nQ7x8fEIDQ2953s3bNgAjUaDl156yWC7n58fPDw8DM6pVqtx8OBB/TlDQ0ORm5uLpKQk/TG7du2CTqdDSEiIMS7tvt15WC2LtImIiMxN9CG2yMhIhIeHIygoCMHBwVi8eDEKCwsxadIkAMCECRPg7e2N6Ohog/fFxMRgxIgRcHFxMdgukUjw9ttv48MPP0T79u3h5+eHuXPnwsvLCyNGjAAAdOzYEUOHDsWUKVOwfPlyaLVaREREYPTo0fDy8jLLddflzkraok80JCIieuCIniC9+OKLyMrKwrx585CRkYFu3bph+/bt+iLr1NRUSKWGSUJKSgr27duHHTt21HjOd955B4WFhZg6dSpyc3PRp08fbN++HdbW1vpj1q5di4iICAwcOBBSqRSjRo3CkiVLTHehDVQ5zV/Jaf5ERERmJ3qCBAARERGIiIiocd/u3burbfP394cgCLWeTyKRYMGCBdXqk+7m7OyMdevWNbit5lKsH2JjDxIREZG58e5rodiDREREJB4mSBZIEARoSlmkTUREJBYmSBaoMjkC+Cw2IiIiMfDua4Eqp/gDfBYbERGRGJggWaDKKf5SCaCQSURuDRER0YOHCZIFunuRSImECRIREZG5MUGyQHcWieTwGhERkRiYIFmgO1P8+fUQERGJgXdgC1TM57ARERGJigmSBdKUsgeJiIhITLwDWyD2IBEREYmLCZIFYg0SERGRuHgHtkCVCRJ7kIiIiMTBBMkC3XkOG78eIiIiMfAObIHYg0RERCQuJkgWqLIHiTVIRERE4uAd2AKxB4mIiEhcTJAs0J0aJCZIREREYmCCZIH0PUgcYiMiIhIF78AWSL8OEnuQiIiIRMEEyQJVrqTNIm0iIiJx8A5sgVikTUREJC4mSBaIRdpERETiYoJkge70IPHrISIiEgPvwBaoWL9QJHuQiIiIxMAEyQJp2INEREQkKt6BLRBrkIiIiMTFBMkC3VkokgkSERGRGJggWaA7C0Xy6yEiIhID78AWqHKhSPYgERERiYMJkoURBAGaUhZpExERiYl3YAujLROgE8r/n89iIyIiEgcTJAtTXNF7BPBZbERERGLhHdjCVBZoSyRMkIiIiMTCO7CF0WgrV9GWQiKRiNwaIiKiBxMTJAtzp0Cb9UdERERiYYJkYYrv6kEiIiIicYh+F166dCl8fX1hbW2NkJAQJCYm3vP43NxcTJs2DZ6enlAqlXj44Yexbds2/X5fX19IJJJqP9OmTdMf069fv2r7X3vtNZNdY0PoV9FmDxIREZFo5GJ++Pr16xEZGYnly5cjJCQEixcvxpAhQ5CSkgI3N7dqx5eUlGDw4MFwc3PDTz/9BG9vb1y9ehWOjo76Yw4dOoSysjszwU6ePInBgwfj+eefNzjXlClTsGDBAv1rlUpl/AtsBP1z2LhIJBERkWhETZAWLVqEKVOmYNKkSQCA5cuXY+vWrVixYgVmzpxZ7fgVK1YgJycH+/fvh0KhAFDeY3S3li1bGrz+5JNP0K5dOzz++OMG21UqFTw8PIx4NcZxpwdJ9M49IiKiB5ZoCVJJSQmSkpIwa9Ys/TapVIpBgwYhISGhxvds2bIFoaGhmDZtGjZv3oyWLVti7NixePfddyGTVe9xKSkpwZo1axAZGVltRtjatWuxZs0aeHh4YPjw4Zg7d+49e5E0Gg00Go3+tVqtBgBotVpotdoGXfvdKt9b+d/C4hIAgJVcel/npeqqxppMh7E2H8bafBhr8zFlrOt7TtESpOzsbJSVlcHd3d1gu7u7O86ePVvjey5duoRdu3Zh3Lhx2LZtGy5cuIA33ngDWq0WUVFR1Y7/+eefkZubi4kTJxpsHzt2LNq0aQMvLy8cP34c7777LlJSUrBx48Za2xsdHY358+dX275jxw6jDM/FxcUBABKzJABkyL/1t0FtFRlPZazJ9Bhr82GszYexNh9TxLqoqKhex4k6xNZQOp0Obm5u+OabbyCTyRAYGIgbN27g008/rTFBiomJwRNPPAEvLy+D7VOnTtX/f0BAADw9PTFw4EBcvHgR7dq1q/GzZ82ahcjISP1rtVoNHx8fhIWFwd7evtHXpNVqERcXh8GDB0OhUCDv0DXgwhm08nTHk092b/R5qbqqsSbTYazNh7E2H8bafEwZ68oRoLqIliC5urpCJpMhMzPTYHtmZmattUGenp5QKBQGw2kdO3ZERkYGSkpKYGVlpd9+9epV7Ny58569QpVCQkIAABcuXKg1QVIqlVAqldW2KxQKo3x5lecp1ZUPBdoojXNeqs5Y3xnVjbE2H8bafBhr8zFFrOt7PtEqga2srBAYGIj4+Hj9Np1Oh/j4eISGhtb4nt69e+PChQvQ6XT6befOnYOnp6dBcgQAK1euhJubG4YNG1ZnW44ePQqgPAETW+Wz2Ky5DhIREZFoRL0LR0ZG4ttvv8Xq1atx5swZvP766ygsLNTPapswYYJBEffrr7+OnJwcTJ8+HefOncPWrVvx8ccfG6xxBJQnWitXrkR4eDjkcsNOsosXL+KDDz5AUlISrly5gi1btmDChAno27cvunTpYvqLroN+oUjOYiMiIhKNqDVIL774IrKysjBv3jxkZGSgW7du2L59u75wOzU1FVLpnUTBx8cHv//+O2bMmIEuXbrA29sb06dPx7vvvmtw3p07dyI1NRUvv/xytc+0srLCzp07sXjxYhQWFsLHxwejRo3CnDlzTHux9aSpnObPdZCIiIhEI3qRdkREBCIiImrct3v37mrbQkNDceDAgXueMywsDIIg1LjPx8cHe/bsaXA7zUW/UCRX0iYiIhINx3EsDBeKJCIiEh/vwhamMkFScoiNiIhINEyQLExlkTZ7kIiIiMTDu7CF0VRM81eyBomIiEg0TJAsjEwqgZVcyiJtIiIiEYk+i40MfRfeU+wmEBERPfDYg0RERERUBRMkIiIioiqYIBERERFVwQSJiIiIqAomSERERERVMEEiIiIiqoIJEhEREVEVTJCIiIiIqmCCRERERFQFEyQiIiKiKpggEREREVXBBImIiIioCiZIRERERFUwQSIiIiKqQi52A5oqQRAAAGq1+r7Oo9VqUVRUBLVaDYVCYYymUS0Ya/NhrM2HsTYfxtp8TBnryvt25X28NkyQGik/Px8A4OPjI3JLiIiIqKHy8/Ph4OBQ636JUFcKRTXS6XRIS0uDnZ0dJBJJo8+jVqvh4+ODa9euwd7e3ogtpKoYa/NhrM2HsTYfxtp8TBlrQRCQn58PLy8vSKW1VxqxB6mRpFIpWrVqZbTz2dvb8y+cmTDW5sNYmw9jbT6MtfmYKtb36jmqxCJtIiIioiqYIBERERFVwQRJZEqlElFRUVAqlWI3pdljrM2HsTYfxtp8GGvzsYRYs0ibiIiIqAr2IBERERFVwQSJiIiIqAomSERERERVMEEiIiIiqoIJkoiWLl0KX19fWFtbIyQkBImJiWI3qcmLjo5Gz549YWdnBzc3N4wYMQIpKSkGxxQXF2PatGlwcXFBixYtMGrUKGRmZorU4ubjk08+gUQiwdtvv63fxlgbz40bN/DSSy/BxcUFNjY2CAgIwOHDh/X7BUHAvHnz4OnpCRsbGwwaNAjnz58XscVNU1lZGebOnQs/Pz/Y2NigXbt2+OCDDwye28VYN97evXsxfPhweHl5QSKR4OeffzbYX5/Y5uTkYNy4cbC3t4ejoyMmT56MgoICo7eVCZJI1q9fj8jISERFRSE5ORldu3bFkCFDcPPmTbGb1qTt2bMH06ZNw4EDBxAXFwetVouwsDAUFhbqj5kxYwZ++eUXbNiwAXv27EFaWhpGjhwpYqubvkOHDuHrr79Gly5dDLYz1sZx69Yt9O7dGwqFAr/99htOnz6Nzz77DE5OTvpjFi5ciCVLlmD58uU4ePAgbG1tMWTIEBQXF4vY8qbnX//6F5YtW4Yvv/wSZ86cwb/+9S8sXLgQX3zxhf4YxrrxCgsL0bVrVyxdurTG/fWJ7bhx43Dq1CnExcXh119/xd69ezF16lTjN1YgUQQHBwvTpk3Tvy4rKxO8vLyE6OhoEVvV/Ny8eVMAIOzZs0cQBEHIzc0VFAqFsGHDBv0xZ86cEQAICQkJYjWzScvPzxfat28vxMXFCY8//rgwffp0QRAYa2N69913hT59+tS6X6fTCR4eHsKnn36q35abmysolUrhhx9+MEcTm41hw4YJL7/8ssG2kSNHCuPGjRMEgbE2JgDCpk2b9K/rE9vTp08LAIRDhw7pj/ntt98EiUQi3Lhxw6jtYw+SCEpKSpCUlIRBgwbpt0mlUgwaNAgJCQkitqz5ycvLAwA4OzsDAJKSkqDVag1i36FDB7Ru3Zqxb6Rp06Zh2LBhBjEFGGtj2rJlC4KCgvD888/Dzc0N3bt3x7fffqvff/nyZWRkZBjE2sHBASEhIYx1Az366KOIj4/HuXPnAADHjh3Dvn378MQTTwBgrE2pPrFNSEiAo6MjgoKC9McMGjQIUqkUBw8eNGp7+LBaEWRnZ6OsrAzu7u4G293d3XH27FmRWtX86HQ6vP322+jduzceeeQRAEBGRgasrKzg6OhocKy7uzsyMjJEaGXTFhsbi+TkZBw6dKjaPsbaeC5duoRly5YhMjISs2fPxqFDh/DWW2/BysoK4eHh+njW9G8KY90wM2fOhFqtRocOHSCTyVBWVoaPPvoI48aNAwDG2oTqE9uMjAy4ubkZ7JfL5XB2djZ6/JkgUbM1bdo0nDx5Evv27RO7Kc3StWvXMH36dMTFxcHa2lrs5jRrOp0OQUFB+PjjjwEA3bt3x8mTJ7F8+XKEh4eL3Lrm5ccff8TatWuxbt06dO7cGUePHsXbb78NLy8vxvoBwyE2Ebi6ukImk1WbzZOZmQkPDw+RWtW8RERE4Ndff8Uff/yBVq1a6bd7eHigpKQEubm5Bscz9g2XlJSEmzdvokePHpDL5ZDL5dizZw+WLFkCuVwOd3d3xtpIPD090alTJ4NtHTt2RGpqKgDo48l/U+7f//3f/2HmzJkYPXo0AgICMH78eMyYMQPR0dEAGGtTqk9sPTw8qk1mKi0tRU5OjtHjzwRJBFZWVggMDER8fLx+m06nQ3x8PEJDQ0VsWdMnCAIiIiKwadMm7Nq1C35+fgb7AwMDoVAoDGKfkpKC1NRUxr6BBg4ciBMnTuDo0aP6n6CgIIwbN07//4y1cfTu3bvachXnzp1DmzZtAAB+fn7w8PAwiLVarcbBgwcZ6wYqKiqCVGp4a5TJZNDpdAAYa1OqT2xDQ0ORm5uLpKQk/TG7du2CTqdDSEiIcRtk1JJvqrfY2FhBqVQKq1atEk6fPi1MnTpVcHR0FDIyMsRuWpP2+uuvCw4ODsLu3buF9PR0/U9RUZH+mNdee01o3bq1sGvXLuHw4cNCaGioEBoaKmKrm4+7Z7EJAmNtLImJiYJcLhc++ugj4fz588LatWsFlUolrFmzRn/MJ598Ijg6OgqbN28Wjh8/LjzzzDOCn5+fcPv2bRFb3vSEh4cL3t7ewq+//ipcvnxZ2Lhxo+Dq6iq88847+mMY68bLz88Xjhw5Ihw5ckQAICxatEg4cuSIcPXqVUEQ6hfboUOHCt27dxcOHjwo7Nu3T2jfvr0wZswYo7eVCZKIvvjiC6F169aClZWVEBwcLBw4cEDsJjV5AGr8Wblypf6Y27dvC2+88Ybg5OQkqFQq4dlnnxXS09PFa3QzUjVBYqyN55dffhEeeeQRQalUCh06dBC++eYbg/06nU6YO3eu4O7uLiiVSmHgwIFCSkqKSK1tutRqtTB9+nShdevWgrW1tdC2bVvhvffeEzQajf4Yxrrx/vjjjxr/jQ4PDxcEoX6x/fvvv4UxY8YILVq0EOzt7YVJkyYJ+fn5Rm+rRBDuWh6UiIiIiFiDRERERFQVEyQiIiKiKpggEREREVXBBImIiIioCiZIRERERFUwQSIiIiKqggkSERERURVMkIiIiIiqYIJERGQkEokEP//8s9jNICIjYIJERM3CxIkTIZFIqv0MHTpU7KYRURMkF7sBRETGMnToUKxcudJgm1KpFKk1RNSUsQeJiJoNpVIJDw8Pgx8nJycA5cNfy5YtwxNPPAEbGxu0bdsWP/30k8H7T5w4gQEDBsDGxgYuLi6YOnUqCgoKDI5ZsWIFOnfuDKVSCU9PT0RERBjsz87OxrPPPguVSoX27dtjy5Ytpr1oIjIJJkhE9MCYO3cuRo0ahWPHjmHcuHEYPXo0zpw5AwAoLCzEkCFD4OTkhEOHDmHDhg3YuXOnQQK0bNkyTJs2DVOnTsWJEyewZcsWPPTQQwafMX/+fLzwwgs4fvw4nnzySYwbNw45OTlmvU4iMgKBiKgZCA8PF2QymWBra2vw89FHHwmCIAgAhNdee83gPSEhIcLrr78uCIIgfPPNN4KTk5NQUFCg379161ZBKpUKGRkZgiAIgpeXl/Dee+/V2gYAwpw5c/SvCwoKBADCb7/9ZrTrJCLzYA0SETUb/fv3x7Jlywy2OTs76/8/NDTUYF9oaCiOHj0KADhz5gy6du0KW1tb/f7evXtDp9MhJSUFEokEaWlpGDhw4D3b0KVLF/3/29rawt7eHjdv3mzsJRGRSJggEVGzYWtrW23Iy1hsbGzqdZxCoTB4LZFIoNPpTNEkIjIh1iAR0QPjwIED1V537NgRANCxY0ccO3YMhYWF+v1//fUXpFIp/P39YWdnB19fX8THx5u1zUQkDvYgEVGzodFokJGRYbBNLpfD1dUVALBhwwYEBQWhT58+WLt2LRITExETEwMAGDduHKKiohAeHo73338fWVlZePPNNzF+/Hi4u7sDAN5//3289tprcHNzwxNPPIH8/Hz89ddfePPNN817oURkckyQiKjZ2L59Ozw9PQ22+fv74+zZswDKZ5jFxsbijTfegKenJ3744Qd06tQJAKBSqfD7779j+vTp6NmzJ1QqFUaNGoVFixbpzxUeHo7i4mJ8/vnn+Oc//wlXV1c899xz5rtAIjIbiSAIgtiNICIyNYlEgk2bNmHEiBFiN4WImgDWIBERERFVwQSJiIiIqArWIBHRA4HVBETUEOxBIiIiIqqCCRIRERFRFUyQiIiIiKpggkRERERUBRMkIiIioiqYIBERERFVwQSJiIiIqAomSERERERV/D8SdEx04S1JyQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "epoch = np.arange(1, len(acc_list) + 1, 1)\n",
    "acc_list = np.array(acc_list)\n",
    "plt.plot(epoch, acc_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e864f84c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch=gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
