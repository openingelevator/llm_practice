{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "579193f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59ab6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "x_data = torch.Tensor([[1.0],[2.0],[3.0]])\n",
    "y_data = torch.Tensor([[0],[0],[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f020a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design model using class\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1,1)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b48257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ab306dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\application\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# Construct optimizer and loss\n",
    "optimizer  = torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "criterion = torch.nn.BCELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77344dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6038],\n",
      "        [0.6564],\n",
      "        [0.7054]], grad_fn=<SigmoidBackward0>)\n",
      "0 2.3429973125457764\n",
      "tensor([[0.5990],\n",
      "        [0.6495],\n",
      "        [0.6969]], grad_fn=<SigmoidBackward0>)\n",
      "1 2.323330879211426\n",
      "tensor([[0.5943],\n",
      "        [0.6428],\n",
      "        [0.6886]], grad_fn=<SigmoidBackward0>)\n",
      "2 2.3049213886260986\n",
      "tensor([[0.5898],\n",
      "        [0.6363],\n",
      "        [0.6805]], grad_fn=<SigmoidBackward0>)\n",
      "3 2.2876932621002197\n",
      "tensor([[0.5854],\n",
      "        [0.6300],\n",
      "        [0.6725]], grad_fn=<SigmoidBackward0>)\n",
      "4 2.2715721130371094\n",
      "tensor([[0.5812],\n",
      "        [0.6239],\n",
      "        [0.6648]], grad_fn=<SigmoidBackward0>)\n",
      "5 2.256488084793091\n",
      "tensor([[0.5770],\n",
      "        [0.6180],\n",
      "        [0.6573]], grad_fn=<SigmoidBackward0>)\n",
      "6 2.242372512817383\n",
      "tensor([[0.5730],\n",
      "        [0.6123],\n",
      "        [0.6501]], grad_fn=<SigmoidBackward0>)\n",
      "7 2.229161500930786\n",
      "tensor([[0.5692],\n",
      "        [0.6067],\n",
      "        [0.6431]], grad_fn=<SigmoidBackward0>)\n",
      "8 2.2167935371398926\n",
      "tensor([[0.5654],\n",
      "        [0.6014],\n",
      "        [0.6362]], grad_fn=<SigmoidBackward0>)\n",
      "9 2.2052102088928223\n",
      "tensor([[0.5618],\n",
      "        [0.5962],\n",
      "        [0.6297]], grad_fn=<SigmoidBackward0>)\n",
      "10 2.194356918334961\n",
      "tensor([[0.5582],\n",
      "        [0.5912],\n",
      "        [0.6233]], grad_fn=<SigmoidBackward0>)\n",
      "11 2.1841812133789062\n",
      "tensor([[0.5548],\n",
      "        [0.5863],\n",
      "        [0.6172]], grad_fn=<SigmoidBackward0>)\n",
      "12 2.174635171890259\n",
      "tensor([[0.5515],\n",
      "        [0.5817],\n",
      "        [0.6113]], grad_fn=<SigmoidBackward0>)\n",
      "13 2.1656718254089355\n",
      "tensor([[0.5483],\n",
      "        [0.5772],\n",
      "        [0.6056]], grad_fn=<SigmoidBackward0>)\n",
      "14 2.1572494506835938\n",
      "tensor([[0.5452],\n",
      "        [0.5729],\n",
      "        [0.6001]], grad_fn=<SigmoidBackward0>)\n",
      "15 2.149327278137207\n",
      "tensor([[0.5422],\n",
      "        [0.5687],\n",
      "        [0.5948]], grad_fn=<SigmoidBackward0>)\n",
      "16 2.1418678760528564\n",
      "tensor([[0.5393],\n",
      "        [0.5647],\n",
      "        [0.5897]], grad_fn=<SigmoidBackward0>)\n",
      "17 2.134836435317993\n",
      "tensor([[0.5365],\n",
      "        [0.5608],\n",
      "        [0.5849]], grad_fn=<SigmoidBackward0>)\n",
      "18 2.1282007694244385\n",
      "tensor([[0.5338],\n",
      "        [0.5571],\n",
      "        [0.5802]], grad_fn=<SigmoidBackward0>)\n",
      "19 2.1219301223754883\n",
      "tensor([[0.5311],\n",
      "        [0.5535],\n",
      "        [0.5757]], grad_fn=<SigmoidBackward0>)\n",
      "20 2.115997076034546\n",
      "tensor([[0.5286],\n",
      "        [0.5501],\n",
      "        [0.5714]], grad_fn=<SigmoidBackward0>)\n",
      "21 2.11037540435791\n",
      "tensor([[0.5261],\n",
      "        [0.5468],\n",
      "        [0.5673]], grad_fn=<SigmoidBackward0>)\n",
      "22 2.1050407886505127\n",
      "tensor([[0.5237],\n",
      "        [0.5436],\n",
      "        [0.5634]], grad_fn=<SigmoidBackward0>)\n",
      "23 2.099971294403076\n",
      "tensor([[0.5214],\n",
      "        [0.5405],\n",
      "        [0.5596]], grad_fn=<SigmoidBackward0>)\n",
      "24 2.0951461791992188\n",
      "tensor([[0.5191],\n",
      "        [0.5376],\n",
      "        [0.5560]], grad_fn=<SigmoidBackward0>)\n",
      "25 2.090545892715454\n",
      "tensor([[0.5170],\n",
      "        [0.5348],\n",
      "        [0.5525]], grad_fn=<SigmoidBackward0>)\n",
      "26 2.086153745651245\n",
      "tensor([[0.5148],\n",
      "        [0.5321],\n",
      "        [0.5492]], grad_fn=<SigmoidBackward0>)\n",
      "27 2.0819528102874756\n",
      "tensor([[0.5128],\n",
      "        [0.5294],\n",
      "        [0.5460]], grad_fn=<SigmoidBackward0>)\n",
      "28 2.0779285430908203\n",
      "tensor([[0.5108],\n",
      "        [0.5269],\n",
      "        [0.5430]], grad_fn=<SigmoidBackward0>)\n",
      "29 2.074066638946533\n",
      "tensor([[0.5088],\n",
      "        [0.5245],\n",
      "        [0.5401]], grad_fn=<SigmoidBackward0>)\n",
      "30 2.070354461669922\n",
      "tensor([[0.5070],\n",
      "        [0.5222],\n",
      "        [0.5374]], grad_fn=<SigmoidBackward0>)\n",
      "31 2.0667805671691895\n",
      "tensor([[0.5051],\n",
      "        [0.5200],\n",
      "        [0.5348]], grad_fn=<SigmoidBackward0>)\n",
      "32 2.06333327293396\n",
      "tensor([[0.5034],\n",
      "        [0.5178],\n",
      "        [0.5323]], grad_fn=<SigmoidBackward0>)\n",
      "33 2.0600039958953857\n",
      "tensor([[0.5016],\n",
      "        [0.5158],\n",
      "        [0.5299]], grad_fn=<SigmoidBackward0>)\n",
      "34 2.0567824840545654\n",
      "tensor([[0.5000],\n",
      "        [0.5138],\n",
      "        [0.5276]], grad_fn=<SigmoidBackward0>)\n",
      "35 2.0536603927612305\n",
      "tensor([[0.4983],\n",
      "        [0.5119],\n",
      "        [0.5254]], grad_fn=<SigmoidBackward0>)\n",
      "36 2.050630569458008\n",
      "tensor([[0.4968],\n",
      "        [0.5101],\n",
      "        [0.5233]], grad_fn=<SigmoidBackward0>)\n",
      "37 2.047685146331787\n",
      "tensor([[0.4952],\n",
      "        [0.5083],\n",
      "        [0.5214]], grad_fn=<SigmoidBackward0>)\n",
      "38 2.044818162918091\n",
      "tensor([[0.4937],\n",
      "        [0.5066],\n",
      "        [0.5195]], grad_fn=<SigmoidBackward0>)\n",
      "39 2.042022705078125\n",
      "tensor([[0.4923],\n",
      "        [0.5050],\n",
      "        [0.5177]], grad_fn=<SigmoidBackward0>)\n",
      "40 2.0392942428588867\n",
      "tensor([[0.4908],\n",
      "        [0.5034],\n",
      "        [0.5160]], grad_fn=<SigmoidBackward0>)\n",
      "41 2.0366268157958984\n",
      "tensor([[0.4894],\n",
      "        [0.5019],\n",
      "        [0.5144]], grad_fn=<SigmoidBackward0>)\n",
      "42 2.0340161323547363\n",
      "tensor([[0.4881],\n",
      "        [0.5005],\n",
      "        [0.5128]], grad_fn=<SigmoidBackward0>)\n",
      "43 2.0314579010009766\n",
      "tensor([[0.4868],\n",
      "        [0.4991],\n",
      "        [0.5114]], grad_fn=<SigmoidBackward0>)\n",
      "44 2.0289483070373535\n",
      "tensor([[0.4855],\n",
      "        [0.4977],\n",
      "        [0.5100]], grad_fn=<SigmoidBackward0>)\n",
      "45 2.0264830589294434\n",
      "tensor([[0.4842],\n",
      "        [0.4964],\n",
      "        [0.5087]], grad_fn=<SigmoidBackward0>)\n",
      "46 2.024059295654297\n",
      "tensor([[0.4830],\n",
      "        [0.4952],\n",
      "        [0.5074]], grad_fn=<SigmoidBackward0>)\n",
      "47 2.0216736793518066\n",
      "tensor([[0.4818],\n",
      "        [0.4940],\n",
      "        [0.5063]], grad_fn=<SigmoidBackward0>)\n",
      "48 2.0193238258361816\n",
      "tensor([[0.4806],\n",
      "        [0.4929],\n",
      "        [0.5051]], grad_fn=<SigmoidBackward0>)\n",
      "49 2.0170063972473145\n",
      "tensor([[0.4795],\n",
      "        [0.4918],\n",
      "        [0.5041]], grad_fn=<SigmoidBackward0>)\n",
      "50 2.0147202014923096\n",
      "tensor([[0.4783],\n",
      "        [0.4907],\n",
      "        [0.5031]], grad_fn=<SigmoidBackward0>)\n",
      "51 2.0124616622924805\n",
      "tensor([[0.4772],\n",
      "        [0.4897],\n",
      "        [0.5021]], grad_fn=<SigmoidBackward0>)\n",
      "52 2.0102295875549316\n",
      "tensor([[0.4762],\n",
      "        [0.4887],\n",
      "        [0.5012]], grad_fn=<SigmoidBackward0>)\n",
      "53 2.008021831512451\n",
      "tensor([[0.4751],\n",
      "        [0.4877],\n",
      "        [0.5004]], grad_fn=<SigmoidBackward0>)\n",
      "54 2.0058364868164062\n",
      "tensor([[0.4741],\n",
      "        [0.4868],\n",
      "        [0.4996]], grad_fn=<SigmoidBackward0>)\n",
      "55 2.0036725997924805\n",
      "tensor([[0.4731],\n",
      "        [0.4859],\n",
      "        [0.4988]], grad_fn=<SigmoidBackward0>)\n",
      "56 2.00152850151062\n",
      "tensor([[0.4721],\n",
      "        [0.4851],\n",
      "        [0.4981]], grad_fn=<SigmoidBackward0>)\n",
      "57 1.9994027614593506\n",
      "tensor([[0.4711],\n",
      "        [0.4843],\n",
      "        [0.4975]], grad_fn=<SigmoidBackward0>)\n",
      "58 1.9972938299179077\n",
      "tensor([[0.4701],\n",
      "        [0.4835],\n",
      "        [0.4969]], grad_fn=<SigmoidBackward0>)\n",
      "59 1.9952008724212646\n",
      "tensor([[0.4692],\n",
      "        [0.4827],\n",
      "        [0.4963]], grad_fn=<SigmoidBackward0>)\n",
      "60 1.9931230545043945\n",
      "tensor([[0.4683],\n",
      "        [0.4820],\n",
      "        [0.4957]], grad_fn=<SigmoidBackward0>)\n",
      "61 1.9910595417022705\n",
      "tensor([[0.4673],\n",
      "        [0.4813],\n",
      "        [0.4952]], grad_fn=<SigmoidBackward0>)\n",
      "62 1.989008903503418\n",
      "tensor([[0.4664],\n",
      "        [0.4806],\n",
      "        [0.4947]], grad_fn=<SigmoidBackward0>)\n",
      "63 1.9869706630706787\n",
      "tensor([[0.4656],\n",
      "        [0.4799],\n",
      "        [0.4943]], grad_fn=<SigmoidBackward0>)\n",
      "64 1.984944462776184\n",
      "tensor([[0.4647],\n",
      "        [0.4793],\n",
      "        [0.4939]], grad_fn=<SigmoidBackward0>)\n",
      "65 1.982928991317749\n",
      "tensor([[0.4639],\n",
      "        [0.4787],\n",
      "        [0.4935]], grad_fn=<SigmoidBackward0>)\n",
      "66 1.9809238910675049\n",
      "tensor([[0.4630],\n",
      "        [0.4781],\n",
      "        [0.4931]], grad_fn=<SigmoidBackward0>)\n",
      "67 1.9789283275604248\n",
      "tensor([[0.4622],\n",
      "        [0.4775],\n",
      "        [0.4928]], grad_fn=<SigmoidBackward0>)\n",
      "68 1.9769420623779297\n",
      "tensor([[0.4614],\n",
      "        [0.4769],\n",
      "        [0.4925]], grad_fn=<SigmoidBackward0>)\n",
      "69 1.9749647378921509\n",
      "tensor([[0.4606],\n",
      "        [0.4764],\n",
      "        [0.4923]], grad_fn=<SigmoidBackward0>)\n",
      "70 1.9729959964752197\n",
      "tensor([[0.4598],\n",
      "        [0.4759],\n",
      "        [0.4920]], grad_fn=<SigmoidBackward0>)\n",
      "71 1.9710350036621094\n",
      "tensor([[0.4590],\n",
      "        [0.4754],\n",
      "        [0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "72 1.9690814018249512\n",
      "tensor([[0.4582],\n",
      "        [0.4749],\n",
      "        [0.4916]], grad_fn=<SigmoidBackward0>)\n",
      "73 1.9671351909637451\n",
      "tensor([[0.4575],\n",
      "        [0.4744],\n",
      "        [0.4914]], grad_fn=<SigmoidBackward0>)\n",
      "74 1.965195894241333\n",
      "tensor([[0.4567],\n",
      "        [0.4739],\n",
      "        [0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "75 1.9632633924484253\n",
      "tensor([[0.4560],\n",
      "        [0.4735],\n",
      "        [0.4911]], grad_fn=<SigmoidBackward0>)\n",
      "76 1.9613370895385742\n",
      "tensor([[0.4552],\n",
      "        [0.4731],\n",
      "        [0.4910]], grad_fn=<SigmoidBackward0>)\n",
      "77 1.9594168663024902\n",
      "tensor([[0.4545],\n",
      "        [0.4726],\n",
      "        [0.4909]], grad_fn=<SigmoidBackward0>)\n",
      "78 1.9575023651123047\n",
      "tensor([[0.4538],\n",
      "        [0.4722],\n",
      "        [0.4908]], grad_fn=<SigmoidBackward0>)\n",
      "79 1.9555935859680176\n",
      "tensor([[0.4531],\n",
      "        [0.4718],\n",
      "        [0.4907]], grad_fn=<SigmoidBackward0>)\n",
      "80 1.9536904096603394\n",
      "tensor([[0.4524],\n",
      "        [0.4715],\n",
      "        [0.4906]], grad_fn=<SigmoidBackward0>)\n",
      "81 1.9517924785614014\n",
      "tensor([[0.4517],\n",
      "        [0.4711],\n",
      "        [0.4906]], grad_fn=<SigmoidBackward0>)\n",
      "82 1.949899673461914\n",
      "tensor([[0.4510],\n",
      "        [0.4707],\n",
      "        [0.4906]], grad_fn=<SigmoidBackward0>)\n",
      "83 1.948011875152588\n",
      "tensor([[0.4503],\n",
      "        [0.4704],\n",
      "        [0.4906]], grad_fn=<SigmoidBackward0>)\n",
      "84 1.9461289644241333\n",
      "tensor([[0.4496],\n",
      "        [0.4700],\n",
      "        [0.4906]], grad_fn=<SigmoidBackward0>)\n",
      "85 1.9442501068115234\n",
      "tensor([[0.4489],\n",
      "        [0.4697],\n",
      "        [0.4906]], grad_fn=<SigmoidBackward0>)\n",
      "86 1.9423764944076538\n",
      "tensor([[0.4483],\n",
      "        [0.4694],\n",
      "        [0.4906]], grad_fn=<SigmoidBackward0>)\n",
      "87 1.9405072927474976\n",
      "tensor([[0.4476],\n",
      "        [0.4691],\n",
      "        [0.4906]], grad_fn=<SigmoidBackward0>)\n",
      "88 1.9386423826217651\n",
      "tensor([[0.4470],\n",
      "        [0.4688],\n",
      "        [0.4907]], grad_fn=<SigmoidBackward0>)\n",
      "89 1.9367821216583252\n",
      "tensor([[0.4463],\n",
      "        [0.4685],\n",
      "        [0.4908]], grad_fn=<SigmoidBackward0>)\n",
      "90 1.9349255561828613\n",
      "tensor([[0.4457],\n",
      "        [0.4682],\n",
      "        [0.4908]], grad_fn=<SigmoidBackward0>)\n",
      "91 1.9330732822418213\n",
      "tensor([[0.4450],\n",
      "        [0.4679],\n",
      "        [0.4909]], grad_fn=<SigmoidBackward0>)\n",
      "92 1.931225061416626\n",
      "tensor([[0.4444],\n",
      "        [0.4676],\n",
      "        [0.4910]], grad_fn=<SigmoidBackward0>)\n",
      "93 1.9293806552886963\n",
      "tensor([[0.4437],\n",
      "        [0.4673],\n",
      "        [0.4911]], grad_fn=<SigmoidBackward0>)\n",
      "94 1.9275404214859009\n",
      "tensor([[0.4431],\n",
      "        [0.4671],\n",
      "        [0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "95 1.925703763961792\n",
      "tensor([[0.4425],\n",
      "        [0.4668],\n",
      "        [0.4913]], grad_fn=<SigmoidBackward0>)\n",
      "96 1.9238712787628174\n",
      "tensor([[0.4419],\n",
      "        [0.4666],\n",
      "        [0.4914]], grad_fn=<SigmoidBackward0>)\n",
      "97 1.9220423698425293\n",
      "tensor([[0.4413],\n",
      "        [0.4663],\n",
      "        [0.4916]], grad_fn=<SigmoidBackward0>)\n",
      "98 1.9202172756195068\n",
      "tensor([[0.4407],\n",
      "        [0.4661],\n",
      "        [0.4917]], grad_fn=<SigmoidBackward0>)\n",
      "99 1.918395757675171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\application\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "# Train cycle\n",
    "for epoch in range(100):\n",
    "    # Predict\n",
    "    y_pred = model(x_data)\n",
    "#     print(y_pred)\n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred,y_data)\n",
    "    print(epoch, loss.item())\n",
    "    \n",
    "    # Clear gradient\n",
    "    optimizer.zero_grad()\n",
    "    # Backpropogate\n",
    "    loss.backward()\n",
    "    # Update\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b2b171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cpu",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
