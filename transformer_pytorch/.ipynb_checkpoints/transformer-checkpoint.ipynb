{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4528, 0.3942, 0.3466, 0.6886],\n",
      "        [0.5443, 0.6389, 0.7202, 0.4414],\n",
      "        [0.4870, 0.6657, 0.8296, 0.2937],\n",
      "        [0.8218, 0.1417, 0.5371, 0.7254]])\n"
     ]
    }
   ],
   "source": [
    "random_torch = torch.rand(4,4)\n",
    "print(random_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[0.0000, 1.1111, 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111, 1.1111, 1.1111]])\n"
     ]
    }
   ],
   "source": [
    "dropout_module = nn.Dropout(p=0.1)\n",
    "tensor_test = torch.ones(5,5)\n",
    "print(tensor_test)\n",
    "print(dropout_module(tensor_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.],\n",
      "        [9.]])\n",
      "tensor([0., 2., 4., 6., 8.])\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  1.5783e-01,  9.8747e-01,  2.5116e-02,\n",
      "          9.9968e-01,  3.9811e-03,  9.9999e-01,  6.3096e-04,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  3.1170e-01,  9.5018e-01,  5.0217e-02,\n",
      "          9.9874e-01,  7.9621e-03,  9.9997e-01,  1.2619e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  4.5775e-01,  8.8908e-01,  7.5285e-02,\n",
      "          9.9716e-01,  1.1943e-02,  9.9993e-01,  1.8929e-03,  1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  5.9234e-01,  8.0569e-01,  1.0031e-01,\n",
      "          9.9496e-01,  1.5924e-02,  9.9987e-01,  2.5238e-03,  1.0000e+00],\n",
      "        [-9.5892e-01,  2.8366e-01,  7.1207e-01,  7.0211e-01,  1.2526e-01,\n",
      "          9.9212e-01,  1.9904e-02,  9.9980e-01,  3.1548e-03,  1.0000e+00],\n",
      "        [-2.7942e-01,  9.6017e-01,  8.1396e-01,  5.8092e-01,  1.5014e-01,\n",
      "          9.8866e-01,  2.3884e-02,  9.9971e-01,  3.7857e-03,  9.9999e-01],\n",
      "        [ 6.5699e-01,  7.5390e-01,  8.9544e-01,  4.4518e-01,  1.7493e-01,\n",
      "          9.8458e-01,  2.7864e-02,  9.9961e-01,  4.4167e-03,  9.9999e-01],\n",
      "        [ 9.8936e-01, -1.4550e-01,  9.5448e-01,  2.9827e-01,  1.9960e-01,\n",
      "          9.7988e-01,  3.1843e-02,  9.9949e-01,  5.0476e-03,  9.9999e-01],\n",
      "        [ 4.1212e-01, -9.1113e-01,  9.8959e-01,  1.4389e-01,  2.2415e-01,\n",
      "          9.7455e-01,  3.5822e-02,  9.9936e-01,  5.6786e-03,  9.9998e-01]])\n"
     ]
    }
   ],
   "source": [
    "encoding = torch.zeros(10, 10)\n",
    "range_tensor = torch.arange(0, 10)\n",
    "print(range_tensor)\n",
    "range_tensor = range_tensor.float().unsqueeze(dim=1)\n",
    "print(range_tensor)\n",
    "_2i = torch.arange(0, 10, step=2).float()\n",
    "print(_2i)\n",
    "encoding[:,0::2] = torch.sin(range_tensor/(10000**(_2i/10)))\n",
    "encoding[:,1::2] = torch.cos(range_tensor/(10000**(_2i/10)))\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将输入的词表索引转化为制定维度的embedding向量\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)\n",
    "\n",
    "\n",
    "class PositionalEmbeddinh(nn.Module):\n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        super(PositionalEmbeddinh, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "        # 位置编码部分无需进行梯度计算\n",
    "        self.encoding.requires_grad = False\n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        return self.encoding[:seq_len, :]\n",
    "    \n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.token_emb = TokenEmbedding(vocab_size, d_model)\n",
    "        self.positional_emb = PositionalEmbeddinh(d_model, max_len, device)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        token_emb = self.token_emb(x)\n",
    "        positional_emb = self.positional_emb(x)\n",
    "        return self.dropout(token_emb + positional_emb)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8602, 0.5952, 0.1883,  ..., 0.0347, 0.8538, 0.3209],\n",
      "         [0.7795, 0.3800, 0.4935,  ..., 0.2866, 0.4380, 0.2638],\n",
      "         [0.0680, 0.6881, 0.8974,  ..., 0.3727, 0.3069, 0.1443],\n",
      "         ...,\n",
      "         [0.9565, 0.8905, 0.7109,  ..., 0.4038, 0.2495, 0.5170],\n",
      "         [0.9452, 0.6876, 0.6367,  ..., 0.0945, 0.9855, 0.9936],\n",
      "         [0.6842, 0.9269, 0.0556,  ..., 0.3792, 0.4920, 0.8326]],\n",
      "\n",
      "        [[0.4641, 0.6400, 0.7785,  ..., 0.9932, 0.6236, 0.7659],\n",
      "         [0.8308, 0.9978, 0.3542,  ..., 0.6326, 0.6208, 0.4169],\n",
      "         [0.6817, 0.8458, 0.7835,  ..., 0.8941, 0.0218, 0.4044],\n",
      "         ...,\n",
      "         [0.4366, 0.7274, 0.2430,  ..., 0.6565, 0.9185, 0.0116],\n",
      "         [0.6958, 0.6547, 0.2299,  ..., 0.9701, 0.6695, 0.0684],\n",
      "         [0.5824, 0.7570, 0.9232,  ..., 0.8923, 0.8773, 0.3020]],\n",
      "\n",
      "        [[0.8023, 0.0818, 0.2763,  ..., 0.7570, 0.8218, 0.1875],\n",
      "         [0.5443, 0.8096, 0.5142,  ..., 0.3124, 0.4487, 0.8635],\n",
      "         [0.5366, 0.0873, 0.1727,  ..., 0.8470, 0.2620, 0.6130],\n",
      "         ...,\n",
      "         [0.2594, 0.1188, 0.4626,  ..., 0.1243, 0.7182, 0.9724],\n",
      "         [0.0404, 0.6100, 0.3873,  ..., 0.2779, 0.3666, 0.8375],\n",
      "         [0.2181, 0.5466, 0.9073,  ..., 0.9232, 0.2895, 0.2873]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.8418, 0.4223, 0.6189,  ..., 0.2676, 0.8240, 0.2222],\n",
      "         [0.7599, 0.5261, 0.2069,  ..., 0.0960, 0.1710, 0.9850],\n",
      "         [0.4455, 0.3876, 0.5509,  ..., 0.1674, 0.6460, 0.8644],\n",
      "         ...,\n",
      "         [0.4918, 0.1789, 0.3494,  ..., 0.2719, 0.4665, 0.7830],\n",
      "         [0.4225, 0.6027, 0.2461,  ..., 0.5582, 0.6050, 0.1282],\n",
      "         [0.9729, 0.3532, 0.1396,  ..., 0.1704, 0.0182, 0.1329]],\n",
      "\n",
      "        [[0.3156, 0.7937, 0.3398,  ..., 0.3192, 0.4353, 0.7026],\n",
      "         [0.9181, 0.6440, 0.6929,  ..., 0.1001, 0.6360, 0.5455],\n",
      "         [0.3208, 0.3171, 0.9527,  ..., 0.7152, 0.2713, 0.4120],\n",
      "         ...,\n",
      "         [0.6235, 0.9420, 0.2488,  ..., 0.4800, 0.2765, 0.7722],\n",
      "         [0.8925, 0.8170, 0.7438,  ..., 0.5411, 0.7087, 0.4202],\n",
      "         [0.8660, 0.5085, 0.7884,  ..., 0.0570, 0.4572, 0.3614]],\n",
      "\n",
      "        [[0.5750, 0.3867, 0.4737,  ..., 0.5227, 0.2786, 0.7649],\n",
      "         [0.4798, 0.3142, 0.9243,  ..., 0.7048, 0.2385, 0.7474],\n",
      "         [0.3651, 0.1946, 0.1857,  ..., 0.0644, 0.6713, 0.6097],\n",
      "         ...,\n",
      "         [0.7345, 0.6783, 0.8983,  ..., 0.1231, 0.9239, 0.8970],\n",
      "         [0.6525, 0.5308, 0.3634,  ..., 0.0658, 0.8426, 0.4800],\n",
      "         [0.1980, 0.8810, 0.3934,  ..., 0.1468, 0.2507, 0.2681]]])\n"
     ]
    }
   ],
   "source": [
    "d_model=512\n",
    "n_head=8\n",
    "seq_len=100\n",
    "batch_size=32\n",
    "x= torch.rand(batch_size,seq_len,d_model)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6652, 0.9868, 0.6111, 0.1540],\n",
      "         [0.1370, 0.5841, 0.9452, 0.7400],\n",
      "         [0.3623, 0.7231, 0.1108, 0.2180],\n",
      "         [0.4648, 0.8470, 0.0261, 0.7433],\n",
      "         [0.9295, 0.9085, 0.0969, 0.5468]],\n",
      "\n",
      "        [[0.6938, 0.1161, 0.5000, 0.2882],\n",
      "         [0.3176, 0.2435, 0.4225, 0.9892],\n",
      "         [0.8092, 0.5260, 0.0355, 0.7638],\n",
      "         [0.5860, 0.4700, 0.4145, 0.9026],\n",
      "         [0.8303, 0.7315, 0.6855, 0.0562]],\n",
      "\n",
      "        [[0.4518, 0.1536, 0.0952, 0.0645],\n",
      "         [0.4997, 0.6820, 0.6177, 0.6402],\n",
      "         [0.3996, 0.2353, 0.1188, 0.7965],\n",
      "         [0.3877, 0.5161, 0.0594, 0.0432],\n",
      "         [0.7465, 0.8587, 0.5447, 0.6269]],\n",
      "\n",
      "        [[0.1015, 0.5006, 0.2319, 0.2783],\n",
      "         [0.5291, 0.6734, 0.0285, 0.6407],\n",
      "         [0.6795, 0.9358, 0.0575, 0.0138],\n",
      "         [0.0529, 0.1513, 0.8129, 0.8101],\n",
      "         [0.1747, 0.1867, 0.0597, 0.9402]],\n",
      "\n",
      "        [[0.2303, 0.6764, 0.4425, 0.1132],\n",
      "         [0.1073, 0.1737, 0.8024, 0.1269],\n",
      "         [0.2553, 0.0803, 0.6521, 0.8847],\n",
      "         [0.8438, 0.9582, 0.3895, 0.7863],\n",
      "         [0.4560, 0.6883, 0.8202, 0.7009]]])\n",
      "tensor([[[0.2547, 0.3513, 0.2413, 0.1528],\n",
      "         [0.1507, 0.2357, 0.3382, 0.2754],\n",
      "         [0.2452, 0.3518, 0.1907, 0.2123],\n",
      "         [0.2257, 0.3307, 0.1455, 0.2981],\n",
      "         [0.3230, 0.3163, 0.1405, 0.2203]],\n",
      "\n",
      "        [[0.3277, 0.1839, 0.2700, 0.2184],\n",
      "         [0.2001, 0.1858, 0.2223, 0.3917],\n",
      "         [0.3154, 0.2376, 0.1455, 0.3014],\n",
      "         [0.2436, 0.2169, 0.2052, 0.3343],\n",
      "         [0.3094, 0.2803, 0.2677, 0.1427]],\n",
      "\n",
      "        [[0.3204, 0.2378, 0.2243, 0.2175],\n",
      "         [0.2234, 0.2681, 0.2514, 0.2571],\n",
      "         [0.2444, 0.2074, 0.1846, 0.3635],\n",
      "         [0.2805, 0.3189, 0.2020, 0.1987],\n",
      "         [0.2616, 0.2926, 0.2138, 0.2321]],\n",
      "\n",
      "        [[0.2073, 0.3090, 0.2362, 0.2474],\n",
      "         [0.2578, 0.2978, 0.1562, 0.2882],\n",
      "         [0.2991, 0.3865, 0.1606, 0.1537],\n",
      "         [0.1569, 0.1731, 0.3355, 0.3345],\n",
      "         [0.1979, 0.2003, 0.1764, 0.4255]],\n",
      "\n",
      "        [[0.2133, 0.3332, 0.2637, 0.1897],\n",
      "         [0.1964, 0.2099, 0.3935, 0.2003],\n",
      "         [0.1922, 0.1613, 0.2858, 0.3606],\n",
      "         [0.2703, 0.3030, 0.1716, 0.2552],\n",
      "         [0.2009, 0.2534, 0.2891, 0.2566]]])\n"
     ]
    }
   ],
   "source": [
    "softmax_module = nn.Softmax(dim=-1)\n",
    "tensor_test = torch.rand(5,5,4)\n",
    "print(tensor_test)\n",
    "print(softmax_module(tensor_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None, e=1e-12):\n",
    "        batch_size, head, seq_len, d_tensor = key.size()\n",
    "        \n",
    "        key_transpose = key.transpose(2, 3)\n",
    "        attn_score = (query@key_transpose)/math.sqrt(d_tensor)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_score = attn_score.masked_fill(mask == 0, -10000)\n",
    "        \n",
    "        attn_score = self.softmax(attn_score)\n",
    "        \n",
    "        value = attn_score@value\n",
    "        \n",
    "        return value, attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.attention = ScaleDotProductAttention()\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_concat = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query, key, value = self.w_q(query), self.w_k(key), self.w_v(value)\n",
    "        \n",
    "        query, key, value = self.split(query), self.split(key), self.split(value)\n",
    "        \n",
    "        out, attention = self.attention(query, key, value, mask)\n",
    "        \n",
    "        out = self.concat(out)\n",
    "        out = self.w_concat(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def split(self, tensor):\n",
    "        batch_size, seq_len, d_model = tensor.size()\n",
    "        d_tensor = d_model // self.n_head\n",
    "        tensor = tensor.view(batch_size, seq_len, self.n_head, d_tensor).transpose(1, 2)\n",
    "        \n",
    "        return tensor\n",
    "    \n",
    "    def concat(self, tensor):\n",
    "        batch_size, head, seq_len, d_tensor = tensor.size()\n",
    "        d_model = head * d_tensor\n",
    "        tensor = tensor.transpose(1,2).contiguous().view(batch_size, seq_len, d_model)\n",
    "\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mark website \n",
    "https://github.com/hyunwoongko/transformer/blob/master/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正则化层\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.std(-1, unbiased=False, keepdim=True)\n",
    "        # print(mean)\n",
    "        # print(var)\n",
    "        # 广播机制\n",
    "        out = (x-mean)/torch.sqrt(var+self.eps)\n",
    "        out = self.gamma*out + self.beta\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5758, 0.3650, 0.7966, 0.0540, 0.3709, 0.3839, 0.6052, 0.8991, 0.8277,\n",
      "         0.3066],\n",
      "        [0.6916, 0.7370, 0.3320, 0.9683, 0.9773, 0.2058, 0.0138, 0.1888, 0.3622,\n",
      "         0.3914],\n",
      "        [0.5048, 0.8577, 0.0768, 0.5691, 0.2114, 0.0060, 0.5521, 0.9818, 0.2747,\n",
      "         0.1764],\n",
      "        [0.8550, 0.7710, 0.7156, 0.3065, 0.3730, 0.8709, 0.0638, 0.9368, 0.4317,\n",
      "         0.8798],\n",
      "        [0.6782, 0.1691, 0.2553, 0.4793, 0.6829, 0.5019, 0.7223, 0.3432, 0.9799,\n",
      "         0.7509]])\n",
      "tensor([[0.5185],\n",
      "        [0.4868],\n",
      "        [0.4211],\n",
      "        [0.6204],\n",
      "        [0.5563]])\n",
      "tensor([[0.2555],\n",
      "        [0.3189],\n",
      "        [0.3110],\n",
      "        [0.2867],\n",
      "        [0.2389]])\n",
      "tensor([[ 0.1134, -0.3037,  0.5502, -0.9189, -0.2920, -0.2662,  0.1715,  0.7530,\n",
      "          0.6119, -0.4192],\n",
      "        [ 0.3626,  0.4430, -0.2742,  0.8526,  0.8685, -0.4976, -0.8376, -0.5277,\n",
      "         -0.2208, -0.1689],\n",
      "        [ 0.1501,  0.7830, -0.6173,  0.2654, -0.3761, -0.7444,  0.2349,  1.0055,\n",
      "         -0.2624, -0.4387],\n",
      "        [ 0.4381,  0.2813,  0.1778, -0.5862, -0.4622,  0.4678, -1.0395,  0.5908,\n",
      "         -0.3524,  0.4844],\n",
      "        [ 0.2494, -0.7923, -0.6159, -0.1575,  0.2590, -0.1113,  0.3396, -0.4360,\n",
      "          0.8667,  0.3982]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 正则化层测试用例\n",
    "norm_module = LayerNorm(10)\n",
    "tensor_test = torch.rand(5,10)\n",
    "print(tensor_test)\n",
    "print(norm_module(tensor_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码层\n",
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        self.ffn = PositionWiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        # 计算自注意力\n",
    "        _x = x\n",
    "        x = self.attention(query=x, key=x, value=x, mask=src_mask)\n",
    "        \n",
    "        # 添加残差连接\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x+_x)\n",
    "        \n",
    "        # 前馈网络\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        # 添加残差连接\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x+_x)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hhk_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
