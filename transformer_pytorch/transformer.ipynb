{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9738, 0.1652, 0.1956, 0.4268],\n",
      "        [0.0213, 0.2562, 0.2386, 0.0656],\n",
      "        [0.5477, 0.9836, 0.1516, 0.8134],\n",
      "        [0.7741, 0.1835, 0.9570, 0.6306]])\n"
     ]
    }
   ],
   "source": [
    "random_torch = torch.rand(4,4)\n",
    "print(random_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[1.1111, 1.1111, 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111, 1.1111, 1.1111]])\n"
     ]
    }
   ],
   "source": [
    "dropout_module = nn.Dropout(p=0.1)\n",
    "tensor_test = torch.ones(5,5)\n",
    "print(tensor_test)\n",
    "print(dropout_module(tensor_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.],\n",
      "        [9.]])\n",
      "tensor([0., 2., 4., 6., 8.])\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  1.5783e-01,  9.8747e-01,  2.5116e-02,\n",
      "          9.9968e-01,  3.9811e-03,  9.9999e-01,  6.3096e-04,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  3.1170e-01,  9.5018e-01,  5.0217e-02,\n",
      "          9.9874e-01,  7.9621e-03,  9.9997e-01,  1.2619e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  4.5775e-01,  8.8908e-01,  7.5285e-02,\n",
      "          9.9716e-01,  1.1943e-02,  9.9993e-01,  1.8929e-03,  1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  5.9234e-01,  8.0569e-01,  1.0031e-01,\n",
      "          9.9496e-01,  1.5924e-02,  9.9987e-01,  2.5238e-03,  1.0000e+00],\n",
      "        [-9.5892e-01,  2.8366e-01,  7.1207e-01,  7.0211e-01,  1.2526e-01,\n",
      "          9.9212e-01,  1.9904e-02,  9.9980e-01,  3.1548e-03,  1.0000e+00],\n",
      "        [-2.7942e-01,  9.6017e-01,  8.1396e-01,  5.8092e-01,  1.5014e-01,\n",
      "          9.8866e-01,  2.3884e-02,  9.9971e-01,  3.7857e-03,  9.9999e-01],\n",
      "        [ 6.5699e-01,  7.5390e-01,  8.9544e-01,  4.4518e-01,  1.7493e-01,\n",
      "          9.8458e-01,  2.7864e-02,  9.9961e-01,  4.4167e-03,  9.9999e-01],\n",
      "        [ 9.8936e-01, -1.4550e-01,  9.5448e-01,  2.9827e-01,  1.9960e-01,\n",
      "          9.7988e-01,  3.1843e-02,  9.9949e-01,  5.0476e-03,  9.9999e-01],\n",
      "        [ 4.1212e-01, -9.1113e-01,  9.8959e-01,  1.4389e-01,  2.2415e-01,\n",
      "          9.7455e-01,  3.5822e-02,  9.9936e-01,  5.6786e-03,  9.9998e-01]])\n"
     ]
    }
   ],
   "source": [
    "encoding = torch.zeros(10, 10)\n",
    "range_tensor = torch.arange(0, 10)\n",
    "print(range_tensor)\n",
    "range_tensor = range_tensor.float().unsqueeze(dim=1)\n",
    "print(range_tensor)\n",
    "_2i = torch.arange(0, 10, step=2).float()\n",
    "print(_2i)\n",
    "encoding[:,0::2] = torch.sin(range_tensor/(10000**(_2i/10)))\n",
    "encoding[:,1::2] = torch.cos(range_tensor/(10000**(_2i/10)))\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将输入的词表索引转化为制定维度的embedding向量\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)\n",
    "\n",
    "\n",
    "class PositionalEmbeddinh(nn.Module):\n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        super(PositionalEmbeddinh, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "        # 位置编码部分无需进行梯度计算\n",
    "        self.encoding.requires_grad = False\n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        return self.encoding[:seq_len, :]\n",
    "    \n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.token_emb = TokenEmbedding(vocab_size, d_model)\n",
    "        self.positional_emb = PositionalEmbeddinh(d_model, max_len, device)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        token_emb = self.token_emb(x)\n",
    "        positional_emb = self.positional_emb(x)\n",
    "        return self.dropout(token_emb + positional_emb)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1718, 0.6347, 0.4844,  ..., 0.0751, 0.4225, 0.8231],\n",
      "         [0.6993, 0.7753, 0.0491,  ..., 0.1735, 0.6124, 0.5339],\n",
      "         [0.5230, 0.8160, 0.9122,  ..., 0.7412, 0.4844, 0.9861],\n",
      "         ...,\n",
      "         [0.8551, 0.2721, 0.2874,  ..., 0.8630, 0.7339, 0.7964],\n",
      "         [0.1180, 0.1976, 0.6389,  ..., 0.8773, 0.7392, 0.7329],\n",
      "         [0.8274, 0.4139, 0.5577,  ..., 0.6363, 0.0265, 0.1200]],\n",
      "\n",
      "        [[0.9705, 0.4886, 0.4199,  ..., 0.2950, 0.8726, 0.1482],\n",
      "         [0.7251, 0.1476, 0.7664,  ..., 0.6897, 0.3211, 0.0617],\n",
      "         [0.1607, 0.3128, 0.7766,  ..., 0.4229, 0.1321, 0.6412],\n",
      "         ...,\n",
      "         [0.0977, 0.2108, 0.8233,  ..., 0.0927, 0.6918, 0.4467],\n",
      "         [0.9970, 0.8300, 0.1543,  ..., 0.4448, 0.7686, 0.4423],\n",
      "         [0.9139, 0.0541, 0.1475,  ..., 0.6415, 0.8883, 0.8334]],\n",
      "\n",
      "        [[0.3270, 0.1988, 0.9699,  ..., 0.3274, 0.8873, 0.0438],\n",
      "         [0.0225, 0.4085, 0.3254,  ..., 0.2133, 0.6447, 0.3613],\n",
      "         [0.6707, 0.4262, 0.9569,  ..., 0.0288, 0.1423, 0.8902],\n",
      "         ...,\n",
      "         [0.7429, 0.5054, 0.9212,  ..., 0.1244, 0.0953, 0.8219],\n",
      "         [0.8589, 0.4247, 0.4712,  ..., 0.2330, 0.2292, 0.8075],\n",
      "         [0.7377, 0.3734, 0.9247,  ..., 0.8140, 0.1507, 0.3838]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3909, 0.1612, 0.1474,  ..., 0.9392, 0.6475, 0.6410],\n",
      "         [0.5251, 0.5353, 0.3723,  ..., 0.0310, 0.2628, 0.4800],\n",
      "         [0.7894, 0.2361, 0.2027,  ..., 0.9863, 0.9596, 0.0669],\n",
      "         ...,\n",
      "         [0.5696, 0.8667, 0.8536,  ..., 0.3571, 0.8920, 0.2443],\n",
      "         [0.5719, 0.4109, 0.3378,  ..., 0.1855, 0.2503, 0.3626],\n",
      "         [0.2585, 0.0680, 0.6937,  ..., 0.3038, 0.3123, 0.4869]],\n",
      "\n",
      "        [[0.2820, 0.8872, 0.3216,  ..., 0.9877, 0.6039, 0.7941],\n",
      "         [0.0357, 0.7672, 0.8366,  ..., 0.4374, 0.9414, 0.5406],\n",
      "         [0.8989, 0.8869, 0.0033,  ..., 0.2750, 0.7108, 0.6838],\n",
      "         ...,\n",
      "         [0.1328, 0.6025, 0.5981,  ..., 0.8939, 0.2614, 0.0528],\n",
      "         [0.5747, 0.4307, 0.5245,  ..., 0.6001, 0.7249, 0.7525],\n",
      "         [0.2938, 0.2604, 0.2908,  ..., 0.2729, 0.6005, 0.3797]],\n",
      "\n",
      "        [[0.3894, 0.8720, 0.0731,  ..., 0.2828, 0.2552, 0.5910],\n",
      "         [0.1996, 0.6263, 0.3216,  ..., 0.7743, 0.2911, 0.3747],\n",
      "         [0.3059, 0.0948, 0.2834,  ..., 0.4059, 0.4808, 0.1716],\n",
      "         ...,\n",
      "         [0.0489, 0.9041, 0.1576,  ..., 0.1532, 0.9464, 0.7311],\n",
      "         [0.1593, 0.2689, 0.5611,  ..., 0.0549, 0.4314, 0.5204],\n",
      "         [0.9697, 0.2799, 0.8088,  ..., 0.0176, 0.6741, 0.3492]]])\n"
     ]
    }
   ],
   "source": [
    "d_model=512\n",
    "n_head=8\n",
    "seq_len=100\n",
    "batch_size=32\n",
    "x= torch.rand(batch_size,seq_len,d_model)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[5.7576e-01, 7.4992e-01, 1.0883e-01, 3.1611e-01],\n",
      "         [8.4574e-01, 2.5657e-01, 7.5338e-01, 7.3449e-01],\n",
      "         [5.2536e-01, 5.6833e-01, 4.2851e-01, 7.0382e-01],\n",
      "         [7.4450e-01, 6.7147e-01, 1.2334e-02, 1.1139e-01],\n",
      "         [4.0722e-01, 1.5480e-01, 6.0856e-01, 7.6757e-01]],\n",
      "\n",
      "        [[2.9967e-01, 5.0381e-01, 8.0809e-01, 7.2294e-01],\n",
      "         [7.6542e-01, 9.3450e-01, 2.5317e-01, 7.7737e-01],\n",
      "         [2.7964e-01, 6.5680e-01, 2.3038e-01, 7.4334e-01],\n",
      "         [5.8598e-01, 7.9420e-01, 9.3936e-01, 2.3606e-01],\n",
      "         [8.0755e-01, 9.2437e-01, 7.3659e-02, 3.7176e-01]],\n",
      "\n",
      "        [[8.4527e-01, 4.7466e-01, 1.9511e-01, 3.6845e-01],\n",
      "         [9.5119e-01, 4.3112e-01, 9.2169e-02, 9.8798e-01],\n",
      "         [2.7633e-01, 1.0228e-01, 8.2631e-01, 4.6585e-01],\n",
      "         [1.6554e-01, 5.4809e-01, 3.8546e-01, 4.2342e-01],\n",
      "         [3.3007e-01, 5.6158e-01, 7.9330e-02, 9.3653e-01]],\n",
      "\n",
      "        [[4.0462e-01, 7.7912e-01, 2.3384e-01, 5.0259e-01],\n",
      "         [9.6370e-01, 3.3861e-01, 1.0660e-01, 7.2118e-01],\n",
      "         [2.3490e-04, 6.2194e-01, 5.9934e-01, 9.8723e-01],\n",
      "         [1.0780e-01, 5.8673e-01, 6.9367e-01, 6.5727e-01],\n",
      "         [7.4165e-01, 5.2171e-01, 6.6712e-01, 9.5527e-01]],\n",
      "\n",
      "        [[6.8934e-01, 1.2013e-01, 7.2575e-02, 3.7964e-01],\n",
      "         [6.9231e-01, 3.5501e-04, 8.5238e-01, 2.7925e-01],\n",
      "         [1.9665e-01, 5.0209e-01, 7.5588e-01, 1.0163e-01],\n",
      "         [2.3849e-01, 2.3480e-01, 3.7856e-01, 8.5829e-01],\n",
      "         [5.9806e-01, 8.9875e-01, 7.6928e-01, 3.5571e-01]]])\n",
      "tensor([[[0.2787, 0.3317, 0.1747, 0.2149],\n",
      "         [0.2975, 0.1651, 0.2713, 0.2662],\n",
      "         [0.2411, 0.2517, 0.2189, 0.2883],\n",
      "         [0.3400, 0.3160, 0.1635, 0.1805],\n",
      "         [0.2255, 0.1752, 0.2758, 0.3234]],\n",
      "\n",
      "        [[0.1846, 0.2264, 0.3070, 0.2819],\n",
      "         [0.2635, 0.3120, 0.1579, 0.2666],\n",
      "         [0.2000, 0.2916, 0.1904, 0.3180],\n",
      "         [0.2294, 0.2824, 0.3266, 0.1616],\n",
      "         [0.3076, 0.3457, 0.1477, 0.1990]],\n",
      "\n",
      "        [[0.3530, 0.2437, 0.1842, 0.2191],\n",
      "         [0.3273, 0.1946, 0.1386, 0.3395],\n",
      "         [0.2091, 0.1757, 0.3624, 0.2527],\n",
      "         [0.1998, 0.2928, 0.2489, 0.2585],\n",
      "         [0.2052, 0.2587, 0.1597, 0.3764]],\n",
      "\n",
      "        [[0.2273, 0.3305, 0.1916, 0.2507],\n",
      "         [0.3644, 0.1950, 0.1546, 0.2859],\n",
      "         [0.1358, 0.2528, 0.2472, 0.3643],\n",
      "         [0.1628, 0.2628, 0.2924, 0.2820],\n",
      "         [0.2520, 0.2022, 0.2339, 0.3120]],\n",
      "\n",
      "        [[0.3522, 0.1993, 0.1901, 0.2584],\n",
      "         [0.2998, 0.1501, 0.3518, 0.1983],\n",
      "         [0.1994, 0.2706, 0.3488, 0.1813],\n",
      "         [0.1998, 0.1991, 0.2298, 0.3713],\n",
      "         [0.2314, 0.3125, 0.2746, 0.1816]]])\n"
     ]
    }
   ],
   "source": [
    "softmax_module = nn.Softmax(dim=-1)\n",
    "tensor_test = torch.rand(5,5,4)\n",
    "print(tensor_test)\n",
    "print(softmax_module(tensor_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None, e=1e-12):\n",
    "        batch_size, head, seq_len, d_tensor = key.size()\n",
    "        \n",
    "        key_transpose = key.transpose(2, 3)\n",
    "        attn_score = (query@key_transpose)/math.sqrt(d_tensor)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_score = attn_score.masked_fill(mask == 0, -10000)\n",
    "        \n",
    "        attn_score = self.softmax(attn_score)\n",
    "        \n",
    "        value = attn_score@value\n",
    "        \n",
    "        return value, attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.attention = ScaleDotProductAttention()\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_concat = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query, key, value = self.w_q(query), self.w_k(key), self.w_v(value)\n",
    "        \n",
    "        query, key, value = self.split(query), self.split(key), self.split(value)\n",
    "        \n",
    "        out, attention = self.attention(query, key, value, mask)\n",
    "        \n",
    "        out = self.concat(out)\n",
    "        out = self.w_concat(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def split(self, tensor):\n",
    "        batch_size, seq_len, d_model = tensor.size()\n",
    "        d_tensor = d_model // self.n_head\n",
    "        tensor = tensor.view(batch_size, seq_len, self.n_head, d_tensor).transpose(1, 2)\n",
    "        \n",
    "        return tensor\n",
    "    \n",
    "    def concat(self, tensor):\n",
    "        batch_size, head, seq_len, d_tensor = tensor.size()\n",
    "        d_model = head * d_tensor\n",
    "        tensor = tensor.transpose(1,2).contiguous().view(batch_size, seq_len, d_model)\n",
    "\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mark website \n",
    "https://github.com/hyunwoongko/transformer/blob/master/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hhk_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
